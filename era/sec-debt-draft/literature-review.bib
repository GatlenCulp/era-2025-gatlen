@online{aguirreKeepFutureHuman2025,
  title = {Keep the {{Future Human}}: {{Why}} and {{How We Should Close}} the {{Gates}} to {{AGI}} and {{Superintelligence}}, and {{What We Should Build Instead}}},
  shorttitle = {Keep the {{Future Human}}},
  author = {Aguirre, Anthony},
  date = {2025-03-07},
  eprint = {2311.09452},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.09452},
  url = {http://arxiv.org/abs/2311.09452},
  urldate = {2025-07-22},
  abstract = {Dramatic advances in artificial intelligence over the past decade (for narrow-purpose AI) and the last several years (for general-purpose AI) have transformed AI from a niche academic field to the core business strategy of many of the world's largest companies, with hundreds of billions of dollars in annual investment in the techniques and technologies for advancing AI's capabilities. We now come to a critical juncture. As the capabilities of new AI systems begin to match and exceed those of humans across many cognitive domains, humanity must decide: how far do we go, and in what direction? This essay argues that we should keep the future human by closing the "gates" to smarter-than-human, autonomous, general-purpose AI -- sometimes called "AGI" -- and especially to the highly-superhuman version sometimes called "superintelligence." Instead, we should focus on powerful, trustworthy AI tools that can empower individuals and transformatively improve human societies' abilities to do what they do best.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computers and Society},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-22T13:18:08.639Z},
  file = {/Users/gat/Zotero/storage/D593RJQ4/Aguirre - 2025 - Keep the Future Human Why and How We Should Close the Gates to AGI and Superintelligence, and What.pdf;/Users/gat/Zotero/storage/J3C6Q6EB/2311.html}
}

@article{AIAcceleratingEcological2025b,
  entrysubtype = {newspaper},
  title = {'{{AI}} Is Accelerating Ecological Disaster, Reinforcing Injustice and Worsening Power Concentration'},
  date = {2025-02-06},
  url = {https://www.lemonde.fr/en/opinion/article/2025/02/06/artificial-intelligence-accelerates-environmental-disaster-deepens-injustices-and-exacerbates-the-concentration-of-power_6737844_23.html},
  urldate = {2025-07-18},
  abstract = {OP-ED. More than 20 organizations in the Hiatus coalition, including La Quadrature du Net and the Ligue des Droits de l'Homme, argue in an opinion piece in Le Monde that the widespread deployment of artificial intelligence must be resisted in the name of human, social and environmental rights.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-18T22:28:00.173Z},
  file = {/Users/gat/Zotero/storage/9U5XJD2E/artificial-intelligence-accelerates-environmental-disaster-deepens-injustices-and-exacerbates-t.html}
}

@online{AIAdoptionGap,
  title = {The {{AI Adoption Gap}}: {{Preparing}} the {{US Government}} for {{Advanced AI}}},
  shorttitle = {The {{AI Adoption Gap}}},
  url = {https://www.forethought.org/research/the-ai-adoption-gap},
  urldate = {2025-07-15},
  abstract = {Advanced AI could unlock an era of enlightened and competent government action. But without smart, active investment, we’ll squander that opportunity and barrel blindly into danger.},
  langid = {english},
  organization = {Forethought},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-15T10:23:29.888Z},
  file = {/Users/gat/Zotero/storage/7NGNM6PC/The AI Adoption Gap Preparing the US Government for Advanced AI.pdf;/Users/gat/Zotero/storage/6JB8PJLQ/the-ai-adoption-gap.html;/Users/gat/Zotero/storage/YEHR2K8B/the-ai-adoption-gap.html}
}

@online{AIBrinkHow2024,
  title = {{{AI}} on the Brink: How Close Are We to Losing Control? - {{IbyIMD}}},
  shorttitle = {{{AI}} on the Brink},
  date = {2024-11-04T08:20:13+00:00},
  url = {https://www.imd.org/ibyimd/artificial-intelligence/ai-on-the-brink-how-close-are-we-to-losing-control/},
  urldate = {2025-07-14},
  abstract = {The AI Safety Clock is set to 29 minutes to midnight, signaling urgent risks of uncontrolled AI. Discover how regulation and global cooperation can mitigate these dangers.},
  langid = {american},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-14T11:17:42.716Z},
  file = {/Users/gat/Zotero/storage/CK89QLW6/ai-on-the-brink-how-close-are-we-to-losing-control.html;/Users/gat/Zotero/storage/J4AA2QDK/ai-on-the-brink-how-close-are-we-to-losing-control.html}
}

@online{AINormalTechnology,
  title = {{{AI}} as {{Normal Technology}}},
  url = {http://knightcolumbia.org/content/ai-as-normal-technology},
  urldate = {2025-07-08},
  langid = {english},
  organization = {Knight First Amendment Institute},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-09T08:29:29.748Z},
  file = {/Users/gat/Zotero/storage/Q94HT5LT/AI as Normal Technology.pdf;/Users/gat/Zotero/storage/3DZ9TU4J/ai-as-normal-technology.html;/Users/gat/Zotero/storage/Q5APLY4K/ai-as-normal-technology.html}
}

@online{AIRisksThat,
  title = {{{AI Risks}} That {{Could Lead}} to {{Catastrophe}} | {{CAIS}}},
  url = {https://safe.ai/ai-risk},
  urldate = {2025-07-17},
  abstract = {There are many potential risks from AI. CAIS focusses on mitigating risks that could lead to catastrophic outcomes for society, such as bioterrorism or loss of control over military AI systems.},
  langid = {english},
  organization = {Center for AI Safety},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-17T14:16:33.539Z},
  file = {/Users/gat/Zotero/storage/Q6BTGVW5/ai-risk.html}
}

@online{amodeiMachinesLovingGrace2024,
  title = {Machines of {{Loving Grace}}},
  author = {Amodei, Dario},
  date = {2024-10},
  url = {https://www.darioamodei.com/essay/machines-of-loving-grace},
  urldate = {2025-07-05},
  abstract = {How AI Could Transform the World for the Better},
  langid = {english},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-05T21:03:09.880Z},
  file = {/Users/gat/Zotero/storage/5CR82M4Y/machines-of-loving-grace.html;/Users/gat/Zotero/storage/N7Q8H7ET/machines-of-loving-grace.html}
}

@online{anthropicAnthropicAwarded$200M2025,
  title = {Anthropic Awarded \${{200M DOD}} Agreement for {{AI}} Capabilities \textbackslash{} {{Anthropic}}},
  author = {{Anthropic}},
  date = {2025-07-14},
  url = {https://www.anthropic.com/news/anthropic-and-the-department-of-defense-to-advance-responsible-ai-in-defense-operations},
  urldate = {2025-07-15},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-15T09:24:50.135Z},
  file = {/Users/gat/Zotero/storage/945KL9ZE/anthropic-and-the-department-of-defense-to-advance-responsible-ai-in-defense-operations.html}
}

@online{anthropicAnthropicEconomicIndex2025,
  title = {Anthropic {{Economic Index}}: {{Insights}} from {{Claude}} 3.7 {{Sonnet}}},
  shorttitle = {Anthropic {{Economic Index}}},
  author = {{Anthropic}},
  date = {2025-03-27},
  url = {https://www.anthropic.com/news/anthropic-economic-index-insights-from-claude-sonnet-3-7},
  urldate = {2025-07-03},
  abstract = {The second update from the Anthropic Economic Index},
  langid = {english},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-03T14:43:27.003Z},
  file = {/Users/gat/Zotero/storage/36ZHFGRX/anthropic-economic-index-insights-from-claude-sonnet-3-7.html;/Users/gat/Zotero/storage/MR66V4SC/anthropic-economic-index-insights-from-claude-sonnet-3-7.html}
}

@online{anthropicAnthropicEconomicIndex2025a,
  title = {Anthropic {{Economic Index}}: {{AI}}'s Impact on Software Development},
  shorttitle = {Anthropic {{Economic Index}}},
  author = {{Anthropic}},
  date = {2025-04-28},
  url = {https://www.anthropic.com/research/impact-software-development},
  urldate = {2025-07-03},
  abstract = {Data on how software developers are using Claude},
  langid = {english},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-03T14:53:11.181Z},
  file = {/Users/gat/Zotero/storage/GCY6KULA/impact-software-development.html;/Users/gat/Zotero/storage/NJPGWQLG/impact-software-development.html}
}

@online{anthropicAnthropicEducationReport2025,
  title = {Anthropic {{Education Report}}: {{How University Students Use Claude}}},
  shorttitle = {Anthropic {{Education Report}}},
  author = {{Anthropic}},
  date = {2025-04-08},
  url = {https://www.anthropic.com/news/anthropic-education-report-how-university-students-use-claude},
  urldate = {2025-07-03},
  abstract = {AI systems are no longer just specialized research tools: they’re everyday academic companions. As AIs integrate more deeply into educational environments, we need to consider important questions about learning, assessment, and skill development. Until now, most discussions have relied on surveys and controlled experiments rather than direct evidence of how students naturally integrate AI into their academic work in real settings.},
  langid = {english},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-12T14:40:41.393Z},
  file = {/Users/gat/Zotero/storage/KEDRTYWQ/anthropic-education-report-how-university-students-use-claude.html;/Users/gat/Zotero/storage/WH42NYTH/anthropic-education-report-how-university-students-use-claude.html}
}

@online{anthropicChallengesEvaluatingAI2023,
  title = {Challenges in Evaluating {{AI}} Systems},
  author = {{Anthropic}},
  date = {2023-10-04},
  url = {https://www.anthropic.com/research/evaluating-ai-systems},
  urldate = {2025-07-03},
  abstract = {Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.},
  langid = {english},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-05T21:07:55.933Z},
  file = {/Users/gat/Zotero/storage/83T5HDWY/evaluating-ai-systems.html;/Users/gat/Zotero/storage/L8VQG6EL/evaluating-ai-systems.html}
}

@online{anthropicClaudeGovModels,
  title = {Claude Gov Models for {{U}}.{{S}}. National Security Customers},
  author = {{Anthropic}},
  url = {https://www.anthropic.com/news/claude-gov-models-for-u-s-national-security-customers},
  urldate = {2025-07-15},
  abstract = {Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.},
  langid = {english},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-15T09:04:06.596Z},
  file = {/Users/gat/Zotero/storage/7A4BTJ48/claude-gov-models-for-u-s-national-security-customers.html}
}

@online{anthropicClioPrivacypreservingInsights2024,
  title = {Clio: {{Privacy-preserving}} Insights into Real-World {{AI}} Use},
  shorttitle = {Clio},
  author = {{Anthropic}},
  date = {2024-12-12},
  url = {https://www.anthropic.com/research/clio},
  urldate = {2025-07-03},
  abstract = {A blog post describing Anthropic’s new system, Clio, for analyzing how people use AI while maintaining their privacy},
  langid = {english},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-03T13:30:00.717Z},
  file = {/Users/gat/Zotero/storage/V799K7BW/clio.html;/Users/gat/Zotero/storage/ZDVYCFS6/clio.html}
}

@online{anthropicCollectiveConstitutionalAI2023,
  title = {Collective {{Constitutional AI}}: {{Aligning}} a {{Language Model}} with {{Public Input}}},
  shorttitle = {Collective {{Constitutional AI}}},
  author = {{Anthropic}},
  date = {2023-10-17},
  url = {https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input},
  urldate = {2025-07-03},
  abstract = {Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.},
  langid = {english},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-05T21:07:30.808Z},
  file = {/Users/gat/Zotero/storage/943WTVK3/collective-constitutional-ai-aligning-a-language-model-with-public-input.html;/Users/gat/Zotero/storage/R4FTZ8KX/collective-constitutional-ai-aligning-a-language-model-with-public-input.html}
}

@online{anthropicEvaluatingFeatureSteering2024,
  title = {Evaluating Feature Steering: {{A}} Case Study in Mitigating Social Biases},
  shorttitle = {Evaluating Feature Steering},
  author = {{Anthropic}},
  date = {2024-10-25},
  url = {https://www.anthropic.com/research/evaluating-feature-steering},
  urldate = {2025-07-03},
  abstract = {A new piece of Anthropic research by Durmus et al.: "Evaluating feature steering: A case study in mitigating social biases"},
  langid = {english},
  annotation = {Read\_Status: Partial Read\\
Read\_Status\_Date: 2025-07-03T14:41:28.950Z},
  file = {/Users/gat/Zotero/storage/569C7BBX/evaluating-feature-steering.html;/Users/gat/Zotero/storage/EU3GL89M/evaluating-feature-steering.html}
}

@online{anthropicEvaluatingMitigatingDiscrimination2023,
  title = {Evaluating and {{Mitigating Discrimination}} in {{Language Model Decisions}}},
  author = {{Anthropic}},
  date = {2023-12-07},
  url = {https://www.anthropic.com/research/evaluating-and-mitigating-discrimination-in-language-model-decisions},
  urldate = {2025-07-03},
  abstract = {Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.},
  langid = {english},
  annotation = {Read\_Status: Partial Read\\
Read\_Status\_Date: 2025-07-03T14:29:09.064Z},
  file = {/Users/gat/Zotero/storage/57YRPIPF/evaluating-and-mitigating-discrimination-in-language-model-decisions.html;/Users/gat/Zotero/storage/69XB93I5/evaluating-and-mitigating-discrimination-in-language-model-decisions.html}
}

@online{anthropicIntroducingAnthropicEconomic2025,
  title = {Introducing the {{Anthropic Economic Index}}},
  author = {{Anthropic}},
  date = {2025-02-10},
  url = {https://www.anthropic.com/news/the-anthropic-economic-index},
  urldate = {2025-07-03},
  abstract = {Announcement of the new Anthropic Economic Index and description of the new data on AI use in occupations},
  langid = {english},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-03T13:55:54.114Z},
  file = {/Users/gat/Zotero/storage/5EA6CNDI/the-anthropic-economic-index.html;/Users/gat/Zotero/storage/7J93XFRW/the-anthropic-economic-index.html}
}

@online{anthropicMeasuringPersuasivenessLanguage2024,
  title = {Measuring the {{Persuasiveness}} of {{Language Models}}},
  author = {{Anthropic}},
  date = {2024-04-09},
  url = {https://www.anthropic.com/research/measuring-model-persuasiveness},
  urldate = {2025-07-03},
  abstract = {Anthropic developed a way to test how persuasive language models (LMs) are, and analyzed how persuasiveness scales across different versions of Claude.},
  langid = {english},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-03T14:01:56.038Z},
  file = {/Users/gat/Zotero/storage/G5VPMJQX/measuring-model-persuasiveness.html;/Users/gat/Zotero/storage/X7QDUYC8/measuring-model-persuasiveness.html}
}

@online{anthropicProjectVendCan2025,
  title = {Project {{Vend}}: {{Can Claude}} Run a Small Shop? ({{And}} Why Does That Matter?)},
  shorttitle = {Project {{Vend}}},
  author = {{Anthropic}},
  date = {2025-06-27},
  url = {https://www.anthropic.com/research/project-vend-1},
  urldate = {2025-07-03},
  abstract = {We let Claude run a small shop in the Anthropic office. Here's what happened.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-03T14:00:01.253Z}
}

@online{anthropicTestingMitigatingElectionsrelated2024,
  title = {Testing and Mitigating Elections-Related Risks},
  author = {{Anthropic}},
  date = {2024-06-06},
  url = {https://www.anthropic.com/news/testing-and-mitigating-elections-related-risks},
  urldate = {2025-07-03},
  abstract = {This blog provides a snapshot of the work we've done since last summer to test our models for elections-related risks.},
  langid = {english},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-05T19:03:37.474Z},
  file = {/Users/gat/Zotero/storage/BAPGLIGS/testing-and-mitigating-elections-related-risks.html;/Users/gat/Zotero/storage/CM3XRKUI/testing-and-mitigating-elections-related-risks.html}
}

@online{anthropicValuesWildDiscovering2025,
  title = {Values in the Wild: {{Discovering}} and Analyzing Values in Real-World Language Model Interactions},
  shorttitle = {Values in the Wild},
  author = {{Anthropic}},
  date = {2025-04-21},
  url = {https://www.anthropic.com/research/values-wild},
  urldate = {2025-07-03},
  abstract = {An Anthropic research paper testing which values AI models express in the real world},
  langid = {english},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-03T14:21:06.450Z},
  file = {/Users/gat/Zotero/storage/CVPGJI5K/values-wild.html;/Users/gat/Zotero/storage/U3MIMZV3/values-wild.html}
}

@article{avelinoTheoriesPowerSocial2021,
  title = {Theories of Power and Social Change. {{Power}} Contestations and Their Implications for Research on Social Change and Innovation},
  author = {Avelino, Flor},
  date = {2021-09-02},
  journaltitle = {Journal of Political Power},
  volume = {14},
  number = {3},
  pages = {425--448},
  publisher = {Routledge},
  issn = {2158-379X},
  doi = {10.1080/2158379X.2021.1875307},
  url = {https://doi.org/10.1080/2158379X.2021.1875307},
  urldate = {2025-07-24},
  abstract = {This paper proposes a meta-theoretical framework for studying power in processes of change and innovation. Power is one of the most contested concepts in social and political theory. This paper discusses seven prevailing points of contestation: Power over versus power to, centred versus diffused, consensual versus conflictual, constraining versus enabling, quantity versus quality, empowerment versus disempowerment and power in relation to knowledge. The paper reviews how different scholars have dealt with abovementioned points of contestation and identifies how different theories of power can be translated into specific empirical questions to systematically explore power in processes of social change and innovation.},
  keywords = {Power,social change,social innovation,sustainability transitions},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-24T13:29:02.753Z},
  file = {/Users/gat/Zotero/storage/XU482W3H/Avelino - 2021 - Theories of power and social change. Power contestations and their implications for research on soci.pdf}
}

@online{AzureGovernmentNational,
  title = {Azure {{Government}} for National Security | {{Microsoft Azure}}},
  url = {https://azure.microsoft.com/en-us/explore/global-infrastructure/government/national-security},
  urldate = {2025-07-15},
  abstract = {Learn about Azure capabilities for US Government classified data},
  langid = {american},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-15T13:59:47.862Z},
  file = {/Users/gat/Zotero/storage/2E8BINCA/national-security.html}
}

@online{bakerNuclearArmsControl2023,
  title = {Nuclear {{Arms Control Verification}} and {{Lessons}} for {{AI Treaties}}},
  author = {Baker, Mauricio},
  date = {2023-04-08},
  eprint = {2304.04123},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.04123},
  url = {http://arxiv.org/abs/2304.04123},
  urldate = {2025-06-12},
  abstract = {Security risks from AI have motivated calls for international agreements that guardrail the technology. However, even if states could agree on what rules to set on AI, the problem of verifying compliance might make these agreements infeasible. To help clarify the difficulty of verifying agreements on AI\$\textbackslash unicode\{x2013\}\$and identify actions that might reduce this difficulty\$\textbackslash unicode\{x2013\}\$this report examines the case study of verification in nuclear arms control. We review the implementation, track records, and politics of verification across three types of nuclear arms control agreements. Then, we consider implications for the case of AI, especially AI development that relies on thousands of highly specialized chips. In this context, the case study suggests that, with certain preparations, the foreseeable challenges of verification would be reduced to levels that were successfully managed in nuclear arms control. To avoid even worse challenges, substantial preparations are needed: (1) developing privacy-preserving, secure, and acceptably priced methods for verifying the compliance of hardware, given inspection access; and (2) building an initial, incomplete verification system, with authorities and precedents that allow its gaps to be quickly closed if and when the political will arises.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computers and Society,Physics - Physics and Society},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-07-03T13:29:10.328Z},
  file = {/Users/gat/Zotero/storage/SH63AUI9/Baker - 2023 - Nuclear Arms Control Verification and Lessons for AI Treaties.pdf;/Users/gat/Zotero/storage/5778UC4F/2304.html}
}

@online{barakAISafetyCourse2025,
  title = {{{AI Safety Course Intro Blog}}},
  author = {Barak, \textasciitilde{} Boaz},
  date = {2025-07-21T02:34:35+00:00},
  url = {https://windowsontheory.org/2025/07/20/ai-safety-course-intro-blog/},
  urldate = {2025-07-21},
  abstract = {I am teaching CS 2881: AI Safety this fall at Harvard. This blog is primarily aimed at students at Harvard or MIT (where we have a cross-registering agreement) who are considering taking the course…},
  langid = {english},
  organization = {Windows On Theory},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-21T08:25:13.554Z},
  file = {/Users/gat/Zotero/storage/R9LWDEFI/ai-safety-course-intro-blog.html}
}

@incollection{beheshti7PrincipalAgent,
  title = {7. {{Principal}} and Agent},
  booktitle = {Bradgate's {{Commercial Law}}},
  author = {Beheshti, Reza and Saintier, Séverine and Thomas, Sean},
  pages = {210--252},
  publisher = {Oxford University Press},
  url = {https://www.oxfordlawtrove.com/display/10.1093/he/9780199284481.001.0001/he-9780199284481-chapter-7?d=%2F10.1093%2Fhe%2F9780199284481.001.0001%2Fhe-9780199284481-chapter-7&p=emailA2ErYEH.rS4oc},
  urldate = {2025-07-21},
  abstract = {This chapter highlights the relationship between principal and agent. It explains that the consent of the principal gives the agent their authority. Principals and agents have certain rights against each other which spring from their agency relationship, most of which may be contractual. The Commercial Agents (Council Directive) Regulations 1993 created rights and obligations for the parties involved in the commercial agency agreement. The chapter then looks at the rights and duties of the agent under general common law. It explains that the withdrawal of consent by either party effectively terminates their relationship and the agent's actual authority to bind the principal.},
  isbn = {978-0-19-186567-1},
  langid = {american},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-21T15:11:54.539Z},
  file = {/Users/gat/Zotero/storage/YENSSCA6/he-9780199284481-chapter-7.html}
}

@article{bockenholtCognitiveMiserResponseModel2012,
  title = {The {{Cognitive-Miser Response Model}}: {{Testing}} for {{Intuitive}} and {{Deliberate Reasoning}}},
  shorttitle = {The {{Cognitive-Miser Response Model}}},
  author = {Bockenholt, Ulf},
  date = {2012-04-01},
  journaltitle = {Psychometrika},
  shortjournal = {Psychometrika},
  volume = {77},
  pages = {388--399},
  doi = {10.1007/s11336-012-9251-y},
  abstract = {In a number of psychological studies, answers to reasoning vignettes have been shown to result from both intuitive and deliberate response processes. This paper utilizes a psychometric model to separate these two response tendencies. An experimental application shows that the proposed model facilitates the analysis of dual-process item responses and the assessment of individual-difference factors, as well as conditions that favor one response tendency over another one.},
  annotation = {Read\_Status: Partial Read\\
Read\_Status\_Date: 2025-07-03T13:56:46.708Z},
  file = {/Users/gat/Zotero/storage/L7BTS2LQ/Bockenholt - 2012 - The Cognitive-Miser Response Model Testing for Intuitive and Deliberate Reasoning.pdf}
}

@online{bostromWhatSingleton2005,
  title = {What Is a {{Singleton}}?},
  author = {Bostrom, Nick},
  date = {2005},
  url = {https://nickbostrom.com/fut/singleton},
  urldate = {2025-07-05},
  annotation = {Read\_Status: Partial Read\\
Read\_Status\_Date: 2025-07-05T19:04:58.684Z},
  file = {/Users/gat/Zotero/storage/DDELIMSF/singleton.html;/Users/gat/Zotero/storage/XWIESHLW/singleton.html}
}

@online{burjaBorrowedOwnedPower2018,
  title = {Borrowed versus {{Owned Power}}},
  author = {Burja, Samo},
  date = {2018-03-23T03:24:19+00:00},
  url = {https://samoburja.com/borrowed-versus-owned-power/},
  urldate = {2025-07-20},
  langid = {american},
  organization = {Samo Burja},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-20T20:16:53.017Z},
  file = {/Users/gat/Zotero/storage/Z4WDYBPR/borrowed-versus-owned-power.html}
}

@article{castellsNetworkTheoryNetwork2011,
  title = {Network {{Theory}}| {{A Network Theory}} of {{Power}}},
  author = {Castells, Manuel},
  date = {2011-04-08},
  journaltitle = {International Journal of Communication},
  volume = {5},
  number = {0},
  pages = {15},
  issn = {1932-8036},
  url = {https://ijoc.org/index.php/ijoc/article/view/1136},
  urldate = {2025-07-21},
  abstract = {Power in the network society is exercised through networks. There are four different forms of power under these social and technological conditions: 1. 	Networking Power: the power of the actors and organizations included in the networks that constitute the core of the global network society over human collectives and  individuals who are not included in these global networks.                                                                          2. 	Network Power:  the power resulting from the standards required to coordinate social interaction in the networks. In this case, power is exercised not by exclusion from the networks but by the imposition of the rules of inclusion. 3. 	Networked Power: the power of social actors over other social actors in the network. The forms and processes of networked power are specific to each network. 4.	Network-making Power: the power to program specific networks according to the interests and values of the programmers, and the power to switch different networks following the strategic alliances between the dominant actors of various networks. Counterpower is exercised in the network society by fighting to change the programs of specific networks and by the effort to disrupt the switches that reflect dominant interests and replace them with alternative switches between networks. Actors are humans, but humans are organized in networks. Human networks act on networks via the programming and switching of organizational networks.  In the network society, power and counterpower aim fundamentally at influencing the neural networks in the human mind by using mass communication networks and mass self-communication networks.},
  issue = {0},
  langid = {english},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-21T11:56:24.827Z},
  file = {/Users/gat/Zotero/storage/SXFVGZBE/Castells - 2011 - Network Theory A Network Theory of Power.pdf}
}

@online{cdaoCDAOAnnouncesPartnerships2025,
  type = {CDAO},
  title = {{{CDAO Announces Partnerships}} with {{Frontier AI Companies}} to {{Address National Security Missio}}},
  author = {{CDAO}},
  date = {2025-07-15},
  url = {https://www.ai.mil/Latest/News-Press/PR-View/Article/4242822/cdao-announces-partnerships-with-frontier-ai-companies-to-address-national-secu/https%3A%2F%2Fwww.ai.mil%2FLatest%2FNews-Press%2FPR-View%2FArticle%2F4242822%2Fcdao-announces-partnerships-with-frontier-ai-companies-to-address-national-secu%2F},
  urldate = {2025-07-15},
  abstract = {The CDAO is partnering with the nation's leading AI labs — Anthropic, Google, OpenAI, and xAI — to address critical national security challenges. These efforts will prototype frontier AI technologies,},
  langid = {american},
  organization = {Chief Digital and Artificial Intelligence Office},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-15T11:18:19.806Z},
  file = {/Users/gat/Zotero/storage/GEB8A9PX/httpswww.ai.html}
}

@online{chessenArtificialIntelligenceWill2016,
  title = {Artificial {{Intelligence}} Will Be the End of Humanity, but Not for the Reasons You Think.},
  author = {Chessen, Matt},
  date = {2016-09-20T21:19:14},
  url = {https://medium.com/short-bytes/artificial-intelligence-will-be-the-end-of-humanity-but-not-for-the-reasons-you-think-482fbfa6858f},
  urldate = {2025-07-02},
  abstract = {AI is the new Frankenstein, the vaguely understood, over-hyped and ominous technological monster meme that threatens to rise up and destroy…},
  langid = {english},
  organization = {Short Bytes},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-03T13:29:24.349Z},
  file = {/Users/gat/Zotero/storage/7S2KS7NC/artificial-intelligence-will-be-the-end-of-humanity-but-not-for-the-reasons-you-think-482fbfa68.html}
}

@online{columbiaRaschModeling2016,
  title = {Rasch {{Modeling}}},
  author = {{Columbia}},
  date = {2016-08-05T16:57:49-04:00},
  url = {https://www.publichealth.columbia.edu/research/population-health-methods/rasch-modeling},
  urldate = {2025-07-02},
  abstract = {Rasch Modeling},
  langid = {english},
  organization = {Columbia University Mailman School of Public Health},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-05T21:09:09.393Z},
  file = {/Users/gat/Zotero/storage/6CG72752/rasch-modeling.html;/Users/gat/Zotero/storage/XD5X6WTD/rasch-modeling.html}
}

@online{critchTASRATaxonomyAnalysis2023,
  title = {{{TASRA}}: A {{Taxonomy}} and {{Analysis}} of {{Societal-Scale Risks}} from {{AI}}},
  shorttitle = {{{TASRA}}},
  author = {Critch, Andrew and Russell, Stuart},
  date = {2023-06-14},
  eprint = {2306.06924},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.06924},
  url = {http://arxiv.org/abs/2306.06924},
  urldate = {2025-07-18},
  abstract = {While several recent works have identified societal-scale and extinction-level risks to humanity arising from artificial intelligence, few have attempted an \{\textbackslash em exhaustive taxonomy\} of such risks. Many exhaustive taxonomies are possible, and some are useful -- particularly if they reveal new risks or practical approaches to safety. This paper explores a taxonomy based on accountability: whose actions lead to the risk, are the actors unified, and are they deliberate? We also provide stories to illustrate how the various risk types could each play out, including risks arising from unanticipated interactions of many AI systems, as well as risks from deliberate misuse, for which combined technical and policy solutions are indicated.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-18T10:38:32.706Z},
  file = {/Users/gat/Zotero/storage/NXZPKDGC/Critch and Russell - 2023 - TASRA a Taxonomy and Analysis of Societal-Scale Risks from AI.pdf;/Users/gat/Zotero/storage/F9YHAKVS/2306.html}
}

@article{daveElonMusksDOGE,
  entrysubtype = {magazine},
  title = {Elon {{Musk}}’s {{DOGE Is Working}} on a {{Custom Chatbot Called GSAi}}},
  author = {Dave, Paresh},
  journaltitle = {Wired},
  issn = {1059-1028},
  url = {https://www.wired.com/story/doge-chatbot-ai-first-agenda/},
  urldate = {2025-07-15},
  abstract = {The chatbot is part of Elon Musk and President Donald Trump’s ambitions to use AI and other technologies to cut costs and modernize the US government.},
  langid = {american},
  keywords = {artificial intelligence,chatbots,doge,elon musk,government,politics},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-15T10:22:54.404Z},
  file = {/Users/gat/Zotero/storage/ZPXZPDGB/doge-chatbot-ai-first-agenda.html}
}

@online{davidsonAIEnabledCoupsHow2025,
  title = {{{AI-Enabled Coups}}: {{How}} a {{Small Group Could Use AI}} to {{Seize Power}}},
  author = {Davidson, Tom and Finnveden, Lukas and Hadshar, Rose},
  date = {2025-04},
  url = {https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power},
  urldate = {2025-07-03},
  abstract = {The development of AI that is more broadly capable than humans will create a new and serious threat: *AI-enabled coups*. An AI-enabled coup could be staged by a very small group, or just a single person, and could occur even in established democracies. Sufficiently advanced AI will introduce three novel dynamics that significantly increase coup risk. Firstly, military and government leaders could fully replace human personnel with AI systems that are *singularly loyal* to them, eliminating the need to gain human supporters for a coup. Secondly, leaders of AI projects could deliberately build AI systems that are *secretly loyal* to them, for example fully autonomous military robots that pass security tests but later execute a coup when deployed in military settings. Thirdly, senior officials within AI projects or the government could gain *exclusive access* to superhuman capabilities in weapons development, strategic planning, persuasion, and cyber offense, and use these to increase their power until they can stage a coup. To address these risks, AI projects should design and enforce rules against AI misuse, audit systems for secret loyalties, and share frontier AI systems with multiple stakeholders. Governments should establish principles for government use of advanced AI, increase oversight of frontier AI projects, and procure AI for critical systems from multiple independent providers.},
  langid = {english},
  organization = {Forethought},
  annotation = {Read\_Status: Partial Read\\
Read\_Status\_Date: 2025-07-12T14:23:03.071Z},
  file = {/Users/gat/Zotero/storage/SKF2S5C6/AI-Enabled Coups How a Small Group Could Use AI to Seize Power.pdf;/Users/gat/Zotero/storage/ENS63YD3/www.forethought.org.html;/Users/gat/Zotero/storage/QAGRTKTJ/www.forethought.org.html}
}

@online{DecisionLabBehavioral,
  title = {The {{Decision Lab}} - {{Behavioral Science}}, {{Applied}}.},
  url = {https://thedecisionlab.com/reference-guide/computer-science/the-ooda-loop},
  urldate = {2025-07-09},
  abstract = {A behavioral design think tank, we apply decision science, digital innovation \& lean methodologies to pressing problems in policy, business \& social justice},
  langid = {american},
  organization = {The Decision Lab},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-09T08:29:21.781Z},
  file = {/Users/gat/Zotero/storage/43CZN872/the-ooda-loop.html;/Users/gat/Zotero/storage/E68YPLWR/the-ooda-loop.html}
}

@online{DHSsResponsibleUse,
  title = {{{DHS}}’s {{Responsible Use}} of {{Generative AI Tools}} | {{Homeland Security}}},
  url = {https://www.dhs.gov/archive/news/2024/12/17/dhss-responsible-use-generative-ai-tools},
  urldate = {2025-07-16},
  abstract = {An announcement on the launch of DHSChat, a new Artificial Intelligence (AI) powered chatbot designed exclusively for internal use within the Department of Homeland Security.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-16T10:19:12.491Z},
  file = {/Users/gat/Zotero/storage/K8LYNBQZ/dhss-responsible-use-generative-ai-tools.html}
}

@article{douglasGradualDisempowermentConcrete2025,
  title = {Gradual {{Disempowerment}}: {{Concrete Research Projects}} — {{AI Alignment Forum}}},
  shorttitle = {Gradual {{Disempowerment}}},
  author = {Douglas, Raymond},
  date = {2025-05-29},
  url = {https://www.alignmentforum.org/posts/GAv4DRGyDHe2orvwB/gradual-disempowerment-concrete-research-projects},
  urldate = {2025-07-17},
  abstract = {This post benefitted greatly from comments, suggestions, and ongoing discussions with David Duvenaud, David Krueger, and Jan Kulveit. All errors are…},
  langid = {english},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-17T15:24:41.785Z},
  file = {/Users/gat/Zotero/storage/NPWUUFES/gradual-disempowerment-concrete-research-projects.html}
}

@online{dragoIntelligenceCurse2025,
  title = {The {{Intelligence Curse}}},
  author = {Drago, Luke and Laine, Rudolf},
  date = {2025-04},
  url = {https://intelligence-curse.ai/},
  urldate = {2025-07-14},
  abstract = {This series examines the incoming crisis of human irrelevance and provides a map towards a future where people remain the masters of their destiny.},
  langid = {english},
  organization = {The Intelligence Curse},
  annotation = {Read\_Status: Partial Read\\
Read\_Status\_Date: 2025-07-15T10:23:15.291Z},
  file = {/Users/gat/Zotero/storage/X5P9KKKE/The Intelligence Curse.pdf;/Users/gat/Zotero/storage/228YIBTF/intelligence-curse.ai.html;/Users/gat/Zotero/storage/IMZE83E6/intelligence-curse.ai.html}
}

@online{durmusMeasuringRepresentationSubjective2024,
  title = {Towards {{Measuring}} the {{Representation}} of {{Subjective Global Opinions}} in {{Language Models}}},
  author = {Durmus, Esin and Nguyen, Karina and Liao, Thomas I. and Schiefer, Nicholas and Askell, Amanda and Bakhtin, Anton and Chen, Carol and Hatfield-Dodds, Zac and Hernandez, Danny and Joseph, Nicholas and Lovitt, Liane and McCandlish, Sam and Sikder, Orowa and Tamkin, Alex and Thamkul, Janel and Kaplan, Jared and Clark, Jack and Ganguli, Deep},
  date = {2024-04-12},
  eprint = {2306.16388},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.16388},
  url = {http://arxiv.org/abs/2306.16388},
  urldate = {2025-07-03},
  abstract = {Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes. When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily become the most similar to the opinions of speakers of those languages. We release our dataset for others to use and build on. Our data is at https://huggingface.co/datasets/Anthropic/llm\_global\_opinions. We also provide an interactive visualization at https://llmglobalvalues.anthropic.com.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {Read\_Status: Partial Read\\
Read\_Status\_Date: 2025-07-03T14:45:03.420Z},
  file = {/Users/gat/Zotero/storage/79FLX2W5/Durmus et al. - 2024 - Towards Measuring the Representation of Subjective Global Opinions in Language Models.pdf;/Users/gat/Zotero/storage/442YEDCX/towards-measuring-the-representation-of-subjective-global-opinions-in-language-models.html;/Users/gat/Zotero/storage/5FXHP3QY/2306.html}
}

@online{eloundouGPTsAreGPTs2023,
  title = {{{GPTs}} Are {{GPTs}}: {{An Early Look}} at the {{Labor Market Impact Potential}} of {{Large Language Models}}},
  shorttitle = {{{GPTs}} Are {{GPTs}}},
  author = {Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock, Daniel},
  date = {2023-08-21},
  eprint = {2303.10130},
  eprinttype = {arXiv},
  eprintclass = {econ},
  doi = {10.48550/arXiv.2303.10130},
  url = {http://arxiv.org/abs/2303.10130},
  urldate = {2025-07-17},
  abstract = {We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80\% of the U.S. workforce could have at least 10\% of their work tasks affected by the introduction of LLMs, while approximately 19\% of workers may see at least 50\% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15\% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56\% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Economics - General Economics,Quantitative Finance - Economics},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-17T11:31:55.669Z},
  file = {/Users/gat/Zotero/storage/CMDLK68D/Eloundou et al. - 2023 - GPTs are GPTs An Early Look at the Labor Market Impact Potential of Large Language Models.pdf;/Users/gat/Zotero/storage/85GNQNPL/2303.html}
}

@article{epperleinEvolutionaryGamesGraphs2015,
  title = {Evolutionary Games on Graphs and Discrete Dynamical Systems},
  author = {Epperlein, Jeremias and Siegmund, Stefan and Stehlík, Petr},
  date = {2015-02-01},
  journaltitle = {Journal of Difference Equations and Applications},
  volume = {21},
  number = {2},
  pages = {72--95},
  publisher = {Taylor \& Francis},
  issn = {1023-6198},
  doi = {10.1080/10236198.2014.988618},
  url = {https://doi.org/10.1080/10236198.2014.988618},
  urldate = {2025-07-22},
  abstract = {Evolutionary games on graphs play an important role in the study of evolution of cooperation in applied biology. Using rigorous mathematical concepts from a dynamical systems and graph theoretical point of view, we formalize the notions of attractor, update rules and update orders. We prove results on attractors for different utility functions and update orders. For complete graphs we characterize attractors for synchronous and sequential update rules. In other cases (for k-regular graphs or for different update orders) we provide sufficient conditions for attractivity of full cooperation and full defection. We construct examples to show that these conditions are not necessary. Finally, by formulating a list of open questions we emphasize the advantages of our rigorous approach.},
  keywords = {05C90,37N25,37N40,91A22,attractors,cooperation,cycles,defection,discrete dynamical systems,evolutionary games on graphs,game theory,non-autonomous dynamical systems},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-22T22:32:02.544Z},
  file = {/Users/gat/Zotero/storage/JWDUPA8D/Epperlein et al. - 2015 - Evolutionary games on graphs and discrete dynamical systems.pdf}
}

@online{erdilMoravecsParadoxIts2024,
  type = {Substack newsletter},
  title = {Moravec’s Paradox and Its Implications},
  author = {Erdil, Ege},
  date = {2024-12-26},
  url = {https://epochai.substack.com/p/moravecs-paradox-and-its-implications-24-12-27},
  urldate = {2025-06-16},
  abstract = {Since the birth of the field of artificial intelligence in the 20th century, researchers have observed that the difficulty of a task for humans at best weakly correlates with its difficulty for AI systems.},
  organization = {Epoch AI},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-07-03T13:30:04.978Z},
  file = {/Users/gat/Zotero/storage/7448FUGI/moravecs-paradox-and-its-implications-24-12-27.html}
}

@online{estesHowSmartToys2025,
  title = {How Smart Do Toys Need to Be, Really?},
  author = {Estes, Adam Clark},
  date = {2025-06-19T11:00:00+00:00},
  url = {https://www.vox.com/technology/417308/chatgpt-ai-barbie-openai-mattel-toys},
  urldate = {2025-07-15},
  abstract = {There’s no need to panic about AI Barbie yet.},
  langid = {american},
  organization = {Vox},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-15T10:45:56.508Z},
  file = {/Users/gat/Zotero/storage/JFNQS5GL/chatgpt-ai-barbie-openai-mattel-toys.html}
}

@online{finnvedenAGILockin2023,
  title = {{{AGI}} and {{Lock-in}}},
  author = {Finnveden, Lukas and Riedel, Jess and Shulman, Carl},
  date = {2023-10},
  url = {https://www.forethought.org/research/agi-and-lock-in},
  urldate = {2025-07-03},
  abstract = {The long-term future of intelligent life is currently unpredictable and undetermined. We argue that the invention of artificial general intelligence (AGI) could change this by making extreme types of lock-in technologically feasible. In particular, we argue that AGI would make it technologically feasible to (i) perfectly preserve nuanced specifications of a wide variety of values or goals far into the future, and (ii) develop AGI-based institutions that would (with high probability) competently pursue any such values for at least millions, and plausibly trillions, of years.},
  langid = {english},
  organization = {Forethought},
  annotation = {Read\_Status: Partial Read\\
Read\_Status\_Date: 2025-07-12T14:22:44.201Z},
  file = {/Users/gat/Zotero/storage/J3XFCXWY/AGI and Lock-in.pdf;/Users/gat/Zotero/storage/3Y3FPGLB/www.forethought.org.html;/Users/gat/Zotero/storage/AK8D6U47/www.forethought.org.html}
}

@book{frenchBasesSocialPower1959,
  title = {The Bases of Social Power},
  author = {French, John and Raven, Bertram},
  date = {1959-01-01},
  journaltitle = {Studies in Social Power},
  volume = {6},
  annotation = {Read\_Status: Partial Read\\
Read\_Status\_Date: 2025-07-21T10:56:01.484Z},
  file = {/Users/gat/Zotero/storage/CMH9KNVU/French and Raven - 1959 - The bases of social power.pdf}
}

@online{GenerativeAITransforming,
  title = {Generative {{AI Transforming}} the {{Public Sector}} | {{Google Cloud}}},
  url = {https://cloud.google.com/blog/topics/public-sector/gemini-at-work-putting-ai-to-work-in-the-public-sector},
  urldate = {2025-07-15},
  abstract = {Google Cloud AI helps revolutionize government services. Enhance productivity, security, and citizen engagement with powerful solutions like Gemini.},
  langid = {english},
  organization = {Google Cloud Blog},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-15T15:08:16.549Z},
  file = {/Users/gat/Zotero/storage/8263CLJI/gemini-at-work-putting-ai-to-work-in-the-public-sector.html}
}

@article{gerlichAIToolsSociety2025,
  title = {{{AI Tools}} in {{Society}}: {{Impacts}} on {{Cognitive Offloading}} and the {{Future}} of {{Critical Thinking}}},
  shorttitle = {{{AI Tools}} in {{Society}}},
  author = {Gerlich, Michael},
  date = {2025-01},
  journaltitle = {Societies},
  volume = {15},
  number = {1},
  pages = {6},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2075-4698},
  doi = {10.3390/soc15010006},
  url = {https://www.mdpi.com/2075-4698/15/1/6},
  urldate = {2025-07-18},
  abstract = {The proliferation of artificial intelligence (AI) tools has transformed numerous aspects of daily life, yet its impact on critical thinking remains underexplored. This study investigates the relationship between AI tool usage and critical thinking skills, focusing on cognitive offloading as a mediating factor. Utilising a mixed-method approach, we conducted surveys and in-depth interviews with 666 participants across diverse age groups and educational backgrounds. Quantitative data were analysed using ANOVA and correlation analysis, while qualitative insights were obtained through thematic analysis of interview transcripts. The findings revealed a significant negative correlation between frequent AI tool usage and critical thinking abilities, mediated by increased cognitive offloading. Younger participants exhibited higher dependence on AI tools and lower critical thinking scores compared to older participants. Furthermore, higher educational attainment was associated with better critical thinking skills, regardless of AI usage. These results highlight the potential cognitive costs of AI tool reliance, emphasising the need for educational strategies that promote critical engagement with AI technologies. This study contributes to the growing discourse on AI’s cognitive implications, offering practical recommendations for mitigating its adverse effects on critical thinking. The findings underscore the importance of fostering critical thinking in an AI-driven world, making this research essential reading for educators, policymakers, and technologists.},
  issue = {1},
  langid = {english},
  keywords = {AI,AI tools,AI trust,artificial intelligence,cognitive development,cognitive offloading,critical thinking,digital dependence,Halpern Critical Thinking Assessment,technology and education},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-18T09:28:23.460Z},
  file = {/Users/gat/Zotero/storage/93948WI2/Gerlich - 2025 - AI Tools in Society Impacts on Cognitive Offloading and the Future of Critical Thinking.pdf}
}

@online{GooglePublicSector,
  title = {Google {{Public Sector Awarded}} \${{200M DoD CDAO Contract}} for {{AI}} \& {{Cloud Acceleration}}},
  url = {https://cloud.google.com/blog/topics/public-sector/google-public-sector-awarded-200-million-contract-to-accelerate-ai-and-cloud-capabilities-across-department-of-defenses-chief-digital-and-artificial-intelligence-office-cdao},
  urldate = {2025-07-15},
  abstract = {Google Public Sector secures \$200M DoD CDAO contract, accelerating AI and cloud adoption to enhance U.S. national security and mission readiness with cutting-edge tech.},
  langid = {english},
  organization = {Google Cloud Blog},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-15T15:08:20.168Z},
  file = {/Users/gat/Zotero/storage/ZA7Q5K63/google-public-sector-awarded-200-million-contract-to-accelerate-ai-and-cloud-capabilities-acros.html}
}

@online{GPI_Workshop_Slides_Houlden_Publicpdf,
  title = {{{GPI}}\_{{Workshop}}\_{{Slides}}\_{{Houlden}}\_{{Public}}.Pdf},
  url = {https://drive.google.com/file/d/1jPG-vSLkje3RmjLk2Yz__ObsGGAGEfHM/view?usp=sharing&usp=embed_facebook},
  urldate = {2025-07-21},
  organization = {Google Docs},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-21T11:22:02.150Z},
  file = {/Users/gat/Zotero/storage/3YRAET4Q/view.html}
}

@online{grahamHHS2024AI2025,
  title = {{{HHS}}’ 2024 {{AI}} Use Case Inventory Shows Move toward Internal Chatbots},
  author = {Graham, Edward},
  date = {2025-01-03},
  url = {https://www.govexec.com/technology/2025/01/hhs-2024-ai-use-case-inventory-shows-move-toward-internal-chatbots/401953/},
  urldate = {2025-07-15},
  abstract = {The agency reported 271 AI use cases in 2024, which it said represented a 66\% increase from its reported 2023 total.},
  langid = {english},
  organization = {Government Executive},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-15T15:08:21.959Z},
  file = {/Users/gat/Zotero/storage/3T8JIBZQ/401953.html}
}

@online{grohBloomsTaxonomyEffective2025,
  title = {Bloom’s {{Taxonomy}} for {{Effective Learning}}: {{Verbs}} for {{Objectives}}},
  shorttitle = {Bloom’s {{Taxonomy}} for {{Effective Learning}}},
  author = {Groh, Kevin},
  date = {2025-01-13T05:20:00+00:00},
  url = {https://www.valamis.com/hub/blooms-taxonomy},
  urldate = {2025-07-12},
  abstract = {Learn what Bloom’s Taxonomy is and the differences between original vs. revised levels. Discover a list of action verbs that you can use to form learning objectives.},
  langid = {american},
  organization = {Valamis},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-12T14:42:25.865Z},
  file = {/Users/gat/Zotero/storage/UJV7BVFN/blooms-taxonomy.html}
}

@article{gutierrezActornetworkTheoryAlgorithms2024,
  title = {On Actor-Network Theory and Algorithms: {{ChatGPT}}~and the New Power Relationships in the Age of {{AI}}},
  shorttitle = {On Actor-Network Theory and Algorithms},
  author = {Gutiérrez, Jorge Luis Morton},
  date = {2024-11-01},
  journaltitle = {AI and Ethics},
  shortjournal = {AI Ethics},
  volume = {4},
  number = {4},
  pages = {1071--1084},
  issn = {2730-5961},
  doi = {10.1007/s43681-023-00314-4},
  url = {https://doi.org/10.1007/s43681-023-00314-4},
  urldate = {2025-07-22},
  abstract = {This research paper examines the intersection of actor-network theory (ANT) and algorithms in the context of artificial intelligence (AI). ANT is a sociological approach that highlights the role of both human and non-human actors in social networks, while algorithms are sets of instructions used to perform specific tasks. The paper argues that the integration of these two concepts, program and translation, can reveal new power dynamics in the age of AI and highlights the importance of understanding these relationships in designing ethical and just AI systems. The paper provides an overview of ANT and algorithms, and analyzes their intersection in the context of AI, particularly regarding the platform of Chat GTP. The paper concludes by proposing a framework for understanding and addressing the power dynamics in AI systems.},
  langid = {english},
  keywords = {Actants,Actors,AI,Artificial Intelligence,Complex Networks,Computational Intelligence,Computational Social Science Social Networks,Computational Social Sciences,Ethical problems,Multiagent Systems,Power relationships},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-22T20:16:59.236Z},
  file = {/Users/gat/Zotero/storage/8UZ9UHNY/Gutiérrez - 2024 - On actor-network theory and algorithms ChatGPT and the new power relationships in the age of AI.pdf}
}

@article{guyerBigTechsPush2025,
  entrysubtype = {newspaper},
  title = {Big {{Tech}}’s Push into Military {{AI}} Is Troubling},
  author = {Guyer, Jonathan},
  date = {2025-06-25},
  journaltitle = {Financial Times},
  url = {https://www.ft.com/content/9751cbe5-e560-4f1a-82ea-9a5899c135a6},
  urldate = {2025-07-17},
  abstract = {Silicon Valley firms are beefing up their national security teams, but scrutiny is sorely needed},
  journalsubtitle = {Artificial intelligence},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-17T12:29:35.392Z},
  file = {/Users/gat/Zotero/storage/QX899MAX/9751cbe5-e560-4f1a-82ea-9a5899c135a6.html}
}

@online{HowMitigateAIdriven,
  title = {How to Mitigate {{AI-driven}} Power Concentration},
  url = {https://futureoflife.org/grant-program/mitigate-ai-driven-power-concentration/},
  urldate = {2025-07-18},
  abstract = {We're offering up to \$4M to support projects that work to mitigate the dangers of AI-driven power concentration and move towards a better world of meaningful human agency.},
  langid = {american},
  organization = {Future of Life Institute},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-18T22:28:46.862Z},
  file = {/Users/gat/Zotero/storage/66Z65FTU/mitigate-ai-driven-power-concentration.html}
}

@online{HumanTakeoverMight,
  title = {Human {{Takeover Might}} Be {{Worse}} than {{AI Takeover}}},
  url = {https://www.forethought.org/research/human-takeover-might-be-worse-than-ai-takeover},
  urldate = {2025-07-14},
  abstract = {AI progress might enable either an AI system or a human with AI assistance to seize power. Which would be worse? In this research note, I present some initial considerations for comparing AI takeover with human takeover. I argue that AI systems will be kinder and more cooperative than humans in expectation, and that conditioning on takeover makes AI takeover more concerning, but by less than you might think. Overall, it’s plausible that human takeover would be worse than AI takeover.},
  langid = {english},
  organization = {Forethought},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-14T11:19:37.720Z},
  file = {/Users/gat/Zotero/storage/73LHFHCB/human-takeover-might-be-worse-than-ai-takeover.html;/Users/gat/Zotero/storage/L8SIB9RJ/human-takeover-might-be-worse-than-ai-takeover.html}
}

@online{IntroducingChatGPTGov,
  title = {Introducing {{ChatGPT Gov}}},
  url = {https://openai.com/global-affairs/introducing-chatgpt-gov/},
  urldate = {2025-07-15},
  abstract = {ChatGPT Gov is designed to streamline government agencies’ access to OpenAI’s frontier models.},
  langid = {american},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-15T12:58:50.622Z},
  file = {/Users/gat/Zotero/storage/94VQ32TX/introducing-chatgpt-gov.html}
}

@online{jrPentagonLaunchesNew2024,
  title = {Pentagon Launches New Generative {{AI}} 'cell' with \${{100M}} for Pilots, Experiments},
  author = {Jr, Sydney J. Freedberg},
  date = {2024-12-11T18:37:07+00:00},
  url = {https://breakingdefense.com/2024/12/pentagon-launches-new-generative-ai-cell-with-100m-for-pilots-experiments/},
  urldate = {2025-07-15},
  abstract = {Taking over for Task Force Lima, the new AI Rapid Capabilities Cell (AIRCC, or “arc”) will test cutting-edge Large Language Models and other GenAI tools for everything from war planning to cybersecurity.},
  langid = {american},
  organization = {Breaking Defense},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-15T14:53:47.482Z},
  file = {/Users/gat/Zotero/storage/D5ZUHVYF/pentagon-launches-new-generative-ai-cell-with-100m-for-pilots-experiments.html}
}

@online{kahnAISafetyAutomation2024,
  title = {{{AI Safety}} and {{Automation Bias}}},
  author = {Kahn, Lauren and Probasco, Emelia and Kinoshita, Ronnie},
  date = {2024-11},
  url = {https://cset.georgetown.edu/publication/ai-safety-and-automation-bias/},
  urldate = {2025-07-02},
  abstract = {Automation bias is a critical issue for artificial intelligence deployment. It can cause otherwise knowledgeable users to make crucial and even obvious errors. Organizational, technical, and educational leaders can mitigate these biases through training, design, and processes. This paper explores automation bias and ways to mitigate it through three case studies: Tesla’s autopilot incidents, aviation incidents at Boeing and Airbus, and Army and Navy air defense incidents.},
  langid = {american},
  organization = {Center for Security and Emerging Technology},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-03T13:30:18.575Z},
  file = {/Users/gat/Zotero/storage/4NNKDT9J/AI Safety and Automation Bias.pdf;/Users/gat/Zotero/storage/Z3PCD62T/ai-safety-and-automation-bias.html}
}

@online{karnofskyAICouldDefeat2022,
  title = {{{AI Could Defeat All Of Us Combined}}},
  author = {Karnofsky, Holden},
  date = {2022-06-09T15:41:22},
  url = {https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/},
  urldate = {2025-07-05},
  abstract = {How big a deal could AI misalignment be? About as big as it gets.},
  langid = {english},
  organization = {Cold Takes},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-08T08:28:20.824Z},
  file = {/Users/gat/Zotero/storage/I3WYJMR6/ai-could-defeat-all-of-us-combined.html;/Users/gat/Zotero/storage/NII8WKU4/ai-could-defeat-all-of-us-combined.html}
}

@online{kelleyFDAUnveilsElsa2025,
  title = {{{FDA}} Unveils ‘{{Elsa}}’ Generative {{AI}} Tool for Staff},
  author = {Kelley, Alexandra},
  date = {2025-06-03},
  url = {https://www.nextgov.com/artificial-intelligence/2025/06/fda-unveils-elsa-generative-ai-tool-staff/405761/},
  urldate = {2025-07-15},
  abstract = {Following a successful pilot, the Food and Drug Administration unveiled its in-house large language model designed to help agency staff in drug clinical evaluations and reviews.},
  langid = {english},
  organization = {Nextgov.com},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-15T10:52:32.604Z},
  file = {/Users/gat/Zotero/storage/2DZZAZFV/405761.html}
}

@online{kokotajloAI20272025,
  title = {{{AI}} 2027},
  author = {Kokotajlo, Daniel and Alexander, Scott and Larsen, Thomas and Lifland, Eli and Dean, Romeo},
  date = {2025-04-03},
  url = {https://ai-2027.com/race},
  urldate = {2025-07-03},
  abstract = {A research-backed AI scenario forecast.},
  langid = {english},
  organization = {AI 2027},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-03T16:03:19.324Z},
  file = {/Users/gat/Zotero/storage/X4WSX2SK/AI 2027.pdf;/Users/gat/Zotero/storage/5Q7TAXN9/race.html;/Users/gat/Zotero/storage/JTTHEIRF/race.html;/Users/gat/Zotero/storage/RBTP46SL/slowdown.html;/Users/gat/Zotero/storage/XHU4MCHF/slowdown.html}
}

@online{kosmynaYourBrainChatGPT2025,
  title = {Your {{Brain}} on {{ChatGPT}}: {{Accumulation}} of {{Cognitive Debt}} When {{Using}} an {{AI Assistant}} for {{Essay Writing Task}}},
  shorttitle = {Your {{Brain}} on {{ChatGPT}}},
  author = {Kosmyna, Nataliya and Hauptmann, Eugene and Yuan, Ye Tong and Situ, Jessica and Liao, Xian-Hao and Beresnitzky, Ashly Vivian and Braunstein, Iris and Maes, Pattie},
  date = {2025-06-10},
  eprint = {2506.08872},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2506.08872},
  url = {http://arxiv.org/abs/2506.08872},
  urldate = {2025-07-02},
  abstract = {This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-03T13:30:12.462Z},
  file = {/Users/gat/Zotero/storage/KGY3EGDF/Kosmyna et al. - 2025 - Your Brain on ChatGPT Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing T.pdf;/Users/gat/Zotero/storage/CH7CHZEY/2506.html}
}

@online{kulveitGradualDisempowermentSystemic2025,
  title = {Gradual {{Disempowerment}}: {{Systemic Existential Risks}} from {{Incremental AI Development}}},
  shorttitle = {Gradual {{Disempowerment}}},
  author = {Kulveit, Jan and Douglas, Raymond and Ammann, Nora and Turan, Deger and Krueger, David and Duvenaud, David},
  date = {2025-01-29},
  eprint = {2501.16946},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.16946},
  url = {http://arxiv.org/abs/2501.16946},
  urldate = {2025-06-14},
  abstract = {This paper examines the systemic risks posed by incremental advancements in artificial intelligence, developing the concept of `gradual disempowerment', in contrast to the abrupt takeover scenarios commonly discussed in AI safety. We analyze how even incremental improvements in AI capabilities can undermine human influence over large-scale systems that society depends on, including the economy, culture, and nation-states. As AI increasingly replaces human labor and cognition in these domains, it can weaken both explicit human control mechanisms (like voting and consumer choice) and the implicit alignments with human interests that often arise from societal systems' reliance on human participation to function. Furthermore, to the extent that these systems incentivise outcomes that do not line up with human preferences, AIs may optimize for those outcomes more aggressively. These effects may be mutually reinforcing across different domains: economic power shapes cultural narratives and political decisions, while cultural shifts alter economic and political behavior. We argue that this dynamic could lead to an effectively irreversible loss of human influence over crucial societal systems, precipitating an existential catastrophe through the permanent disempowerment of humanity. This suggests the need for both technical research and governance approaches that specifically address the risk of incremental erosion of human influence across interconnected societal systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computers and Society},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-05T17:52:57.176Z},
  file = {/Users/gat/Zotero/storage/5Y5TB64W/Kulveit et al. - 2025 - Gradual Disempowerment Systemic Existential Risks from Incremental AI Development.pdf;/Users/gat/Zotero/storage/4K3BUYD9/2501.html}
}

@online{laineHistoryFuture202520272025,
  title = {A {{History}} of the {{Future}}, 2025-2027},
  author = {Laine, Rudolf},
  date = {2025-02-17},
  url = {https://www.nosetgauge.com/p/a-history-of-the-future-2025-2027},
  urldate = {2025-07-20},
  abstract = {A scenario, part 1.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-20T18:56:34.517Z},
  file = {/Users/gat/Zotero/storage/MMG8AVYR/a-history-of-the-future-2025-2027.html}
}

@online{LawAILaw,
  title = {The {{Law}} of {{AI}} Is the {{Law}} of {{Risky Agents Without Intentions}} | {{The University}} of {{Chicago Law Review}}},
  url = {https://lawreview.uchicago.edu/online-archive/law-ai-law-risky-agents-without-intentions},
  urldate = {2025-07-21},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-21T14:48:36.603Z},
  file = {/Users/gat/Zotero/storage/VMU5W847/law-ai-law-risky-agents-without-intentions.html}
}

@online{liangWidespreadAdoptionLarge2025,
  title = {The {{Widespread Adoption}} of {{Large Language Model-Assisted Writing Across Society}}},
  author = {Liang, Weixin and Zhang, Yaohui and Codreanu, Mihai and Wang, Jiayu and Cao, Hancheng and Zou, James},
  date = {2025-02-17},
  eprint = {2502.09747},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.09747},
  url = {http://arxiv.org/abs/2502.09747},
  urldate = {2025-07-15},
  abstract = {The recent advances in large language models (LLMs) attracted significant public and policymaker interest in its adoption patterns. In this paper, we systematically analyze LLM-assisted writing across four domains-consumer complaints, corporate communications, job postings, and international organization press releases-from January 2022 to September 2024. Our dataset includes 687,241 consumer complaints, 537,413 corporate press releases, 304.3 million job postings, and 15,919 United Nations (UN) press releases. Using a robust population-level statistical framework, we find that LLM usage surged following the release of ChatGPT in November 2022. By late 2024, roughly 18\% of financial consumer complaint text appears to be LLM-assisted, with adoption patterns spread broadly across regions and slightly higher in urban areas. For corporate press releases, up to 24\% of the text is attributable to LLMs. In job postings, LLM-assisted writing accounts for just below 10\% in small firms, and is even more common among younger firms. UN press releases also reflect this trend, with nearly 14\% of content being generated or modified by LLMs. Although adoption climbed rapidly post-ChatGPT, growth appears to have stabilized by 2024, reflecting either saturation in LLM adoption or increasing subtlety of more advanced models. Our study shows the emergence of a new reality in which firms, consumers and even international organizations substantially rely on generative AI for communications.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Computation and Language},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-15T10:51:55.747Z},
  file = {/Users/gat/Zotero/storage/GZKHJACI/Liang et al. - 2025 - The Widespread Adoption of Large Language Model-Assisted Writing Across Society.pdf;/Users/gat/Zotero/storage/V98LUXKA/2502.html}
}

@article{macinnesAnarchyArchitectCompetitive2024,
  title = {Anarchy as {{Architect}}: {{Competitive Pressure}}, {{Technology}}, and the {{Internal Structure}} of {{States}}},
  shorttitle = {Anarchy as {{Architect}}},
  author = {MacInnes, Morgan and Garfinkel, Ben and Dafoe, Allan},
  date = {2024-12-01},
  journaltitle = {International Studies Quarterly},
  shortjournal = {International Studies Quarterly},
  volume = {68},
  number = {4},
  pages = {sqae111},
  issn = {0020-8833},
  doi = {10.1093/isq/sqae111},
  url = {https://doi.org/10.1093/isq/sqae111},
  urldate = {2025-07-20},
  abstract = {The internal institutional structures of states greatly impact their citizens’ welfare. However, states are not at complete liberty to adopt any internal form. Competitive pressure arising from anarchy limits the range of viable domestic institutions to those that do not impose a significant disadvantage. We argue that technological change can alter the relative competitiveness of different state forms and, by extension, improve or degrade human welfare. We empirically support this argument through a macrohistorical survey of competitively significant technologies. We conclude that the true costs of international anarchy are greater than commonly appreciated, as competitive pressure may force states to evolve into forms detrimental to the welfare of their inhabitants. Moreover, the adoption of state forms that improve human well-being is often driven by technological change as much as human agency. Finally, the invention of seemingly beneficial technologies may decrease human well-being by improving the competitiveness of inegalitarian state forms.},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-20T18:47:45.075Z},
  file = {/Users/gat/Zotero/storage/63X84YLF/MacInnes et al. - 2024 - Anarchy as Architect Competitive Pressure, Technology, and the Internal Structure of States.pdf;/Users/gat/Zotero/storage/D8CJQPD8/sqae111.html}
}

@online{maDataBottleneckWhere2025,
  type = {Substack newsletter},
  title = {The {{Data Bottleneck}}: {{Where AGI Stalls}}—and {{Governance Begins}}},
  shorttitle = {The {{Data Bottleneck}}},
  author = {Ma, Michelle},
  date = {2025-05-19},
  url = {https://bullishlemon.substack.com/p/the-data-bottleneck-where-agi-stallsand},
  urldate = {2025-07-10},
  abstract = {Long-form workflow data is scarce...and that might be fantastic for safety},
  organization = {Bullish Lemon},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-07-15T09:51:10.331Z},
  file = {/Users/gat/Zotero/storage/43BSNY7M/the-data-bottleneck-where-agi-stallsand.html}
}

@online{malderHowRiskyChatGPT2024,
  title = {How Risky Is {{ChatGPT}}? {{Depends}} Which Federal Agency You Ask},
  shorttitle = {How Risky Is {{ChatGPT}}?},
  author = {{malder}},
  date = {2024-02-05T17:20:57+00:00},
  url = {https://fedscoop.com/how-risky-is-chatgpt-depends-which-federal-agency-you-ask/},
  urldate = {2025-07-16},
  abstract = {A majority of civilian CFO Act agencies have come up with generative AI strategies, according to a FedScoop analysis.},
  langid = {american},
  organization = {FedScoop},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-16T14:32:00.183Z},
  file = {/Users/gat/Zotero/storage/3D2WBN35/how-risky-is-chatgpt-depends-which-federal-agency-you-ask.html}
}

@online{malderTranslationEmailDrafting2024,
  title = {From Translation to Email Drafting, {{State Department}} Turns to {{AI}} to Assist Workforce},
  author = {{malder}},
  date = {2024-12-11T19:39:04+00:00},
  url = {https://fedscoop.com/state-department-ai-chatbot-email-drafting-northstar-famsearch/},
  urldate = {2025-07-15},
  abstract = {The agency responsible for foreign affairs has introduced an internal chatbot and tools to aid employees with things like news analysis and searching department policy.},
  langid = {american},
  organization = {FedScoop},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-15T17:59:53.558Z},
  file = {/Users/gat/Zotero/storage/8FEXP4FF/state-department-ai-chatbot-email-drafting-northstar-famsearch.html}
}

@online{mbrackenMicrosoftsAzureOpenAI2024,
  title = {Microsoft’s {{Azure OpenAI Service}} Lands {{FedRAMP High}} Authorization},
  author = {{mbracken}},
  date = {2024-08-12T13:01:00+00:00},
  url = {https://fedscoop.com/microsoft-azure-openai-service-fedramp/},
  urldate = {2025-07-16},
  abstract = {Included in the tech giant’s leading artificial intelligence product will be its GPT-4o model.},
  langid = {american},
  organization = {FedScoop},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-16T14:32:12.594Z},
  file = {/Users/gat/Zotero/storage/AV8IDWU7/microsoft-azure-openai-service-fedramp.html}
}

@online{mcstayMattelOpenAIHave2025,
  title = {Mattel and {{OpenAI}} Have Partnered up – Here’s Why Parents Should Be Concerned about {{AI}} in Toys},
  author = {McStay, Andrew},
  date = {2025-06-25},
  url = {http://theconversation.com/mattel-and-openai-have-partnered-up-heres-why-parents-should-be-concerned-about-ai-in-toys-259500},
  urldate = {2025-07-15},
  abstract = {What happens when your child’s toy appears to care for them – but doesn’t really?},
  langid = {british},
  organization = {The Conversation},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-15T10:45:25.489Z}
}

@article{MeasuringImpactEarly20252025,
  title = {Measuring the {{Impact}} of {{Early-2025 AI}} on {{Experienced Open-Source Developer Productivity}}},
  date = {2025-07-10},
  journaltitle = {METR Blog},
  url = {https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/},
  urldate = {2025-07-11},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-11T08:55:06.978Z},
  file = {/Users/gat/Zotero/storage/EY9KP9EN/2025 - Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity.pdf;/Users/gat/Zotero/storage/4LQ7K9YJ/2025-07-10-early-2025-ai-experienced-os-dev-study.html;/Users/gat/Zotero/storage/VDPCFLYS/2025-07-10-early-2025-ai-experienced-os-dev-study.html}
}

@online{murphyGeneratingWargames2025,
  type = {Substack newsletter},
  title = {Generating {{Wargames}}},
  author = {Murphy, Dennis},
  date = {2025-04-13},
  url = {https://dennismurphy.substack.com/p/generating-wargames},
  urldate = {2025-07-05},
  abstract = {Leveraging LLMs for Red Team Emulation - PME, Routine Diplomacy, and Impersonation},
  organization = {Atlanta Grand Strategy},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-05T19:48:57.314Z},
  file = {/Users/gat/Zotero/storage/62IMK6SK/generating-wargames.html;/Users/gat/Zotero/storage/IS6KKBSU/generating-wargames.html}
}

@online{OnceAIResearch,
  title = {Once {{AI Research}} Is {{Automated}}, {{Will AI Progress Accelerate}}?},
  url = {https://www.forethought.org/research/once-ai-research-is-automated-will-ai-progress-accelerate},
  urldate = {2025-07-14},
  abstract = {Today, AI progress is driven by humans, and the rate of progress is roughly constant over time. But once AI itself drives AI progress, this feedback loop could cause the rate of AI progress to accelerate, getting faster and faster over time. In this piece, we explain the conditions under which progress accelerates, and then evaluate whether these conditions hold for three feedback loops by which AI will improve AI: software, chip technology, and chip production. Setting human constraints aside, we argue that software might sustain accelerating progress, chip technology is likely to , and chip production is very likely to sustain accelerating progress. Overall AI progress is even more likely to accelerate, as it can involve all three feedback loops.},
  langid = {english},
  organization = {Forethought},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-21T11:14:32.908Z},
  file = {/Users/gat/Zotero/storage/K8PGZT5I/Once AI Research is Automated, Will AI Progress Accelerate.pdf;/Users/gat/Zotero/storage/HYHCHJ6U/once-ai-research-is-automated-will-ai-progress-accelerate.html;/Users/gat/Zotero/storage/UTAA77E6/once-ai-research-is-automated-will-ai-progress-accelerate.html}
}

@online{openaiIntroducingOpenAIGovernment2025,
  title = {Introducing {{OpenAI}} for {{Government}}},
  author = {{OpenAI}},
  date = {2025-06-16},
  url = {https://openai.com/global-affairs/introducing-openai-for-government/},
  urldate = {2025-07-15},
  abstract = {We’re launching OpenAI for Government, a new initiative focused on bringing our most advanced AI tools to public servants across the United States. We're supporting the U.S. government's efforts in adopting best-in-class technology and deploying these tools in service of the public good.},
  langid = {american},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-15T09:23:37.036Z},
  file = {/Users/gat/Zotero/storage/RC3QURHN/introducing-openai-for-government.html}
}

@online{openaiStrengtheningAmericasAI2025,
  title = {Strengthening {{America}}’s {{AI}} Leadership with the {{U}}.{{S}}. {{National Laboratories}}},
  author = {{OpenAI}},
  date = {2025-01-30},
  url = {https://openai.com/index/strengthening-americas-ai-leadership-with-the-us-national-laboratories/},
  urldate = {2025-07-15},
  abstract = {OpenAI’s latest line of reasoning models will be used by nation’s leading scientists to drive scientific breakthroughs.},
  langid = {american},
  organization = {OpenAI},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-15T14:27:17.000Z},
  file = {/Users/gat/Zotero/storage/T7VS8MB8/strengthening-americas-ai-leadership-with-the-us-national-laboratories.html}
}

@article{owencbDecomposingAgencyCapabilities2024,
  title = {Decomposing {{Agency}} — Capabilities without Desires},
  author = {{owencb} and Douglas, Raymond},
  date = {2024-07-11},
  url = {https://www.lesswrong.com/posts/jpGHShgevmmTqXHy5/decomposing-agency-capabilities-without-desires},
  urldate = {2025-07-20},
  abstract = {People sometimes say that AGI will be like a second species; sometimes like electricity. Both may have elements of truth. We need concepts which let us think clearly about that region in-between the two.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-20T19:36:11.966Z},
  file = {/Users/gat/Zotero/storage/W2K5JN2D/decomposing-agency-capabilities-without-desires.html}
}

@online{owenChallengesPredictingAI2023,
  title = {Challenges in {{Predicting AI Automation}}},
  author = {Owen, David},
  date = {2023-11-24T00:00:00+00:00},
  url = {https://epoch.ai/blog/challenges-in-predicting-ai-automation},
  urldate = {2025-07-03},
  abstract = {Economists propose various approaches to predicting AI’s automation of valuable tasks, but disagreements persist, with no consensus on the best method.},
  langid = {english},
  organization = {Epoch AI},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-07-03T13:32:36.082Z},
  file = {/Users/gat/Zotero/storage/MGXJ47T7/challenges-in-predicting-ai-automation.html;/Users/gat/Zotero/storage/RCDAYUE7/challenges-in-predicting-ai-automation.html}
}

@online{panzerImperfectRecallAI2024,
  title = {Imperfect {{Recall}} and {{AI Delegation}} - {{Eric Olav Chen}}, {{Alexis Ghersengorin}} and {{Sami Petersen}}},
  author = {Panzer, Christian},
  date = {2024-11-28T08:49:43+00:00},
  url = {https://globalprioritiesinstitute.org/imperfect-recall-and-ai-delegation-chen-ghersengorin-and-petersen/},
  urldate = {2025-07-14},
  abstract = {A principal wants to deploy an artificial intelligence (AI) system to perform some task. But the AI may be misaligned and aim to pursue a conflicting objective. The principal cannot restrict its options or deliver punishments. Instead, the principal is endowed with the ability to impose imperfect recall on the agent. The principal can then simulate the task and obscure whether it is real or part of a test. This allows the principal to screen misaligned AIs during testing and discipline their behaviour in deployment. By increasing the number of tests, the principal can screen arbitrarily well and may even discipline perfectly in finite time. We show that, in equilibrium, screening can only be achieved with imperfect recall. The perfect screening result is robust to the agent observing any amount of noisy information revealing the nature of the task.},
  langid = {british},
  organization = {Global Priorities Institute},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-14T13:03:02.331Z},
  file = {/Users/gat/Zotero/storage/97UTCE3R/imperfect-recall-and-ai-delegation-chen-ghersengorin-and-petersen.html}
}

@online{perrigoBlueprintRedistributingAIs2025,
  title = {A {{Blueprint}} for {{Redistributing AI}}’s {{Profits}}},
  author = {Perrigo, Billy},
  date = {2025-07-11T14:07:33},
  url = {https://time.com/7301736/ai-redistribution-profits/},
  urldate = {2025-07-16},
  abstract = {A new paper suggests a novel way for states to protect their populations in a world of mass AI-enabled job loss.},
  langid = {english},
  organization = {TIME},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-16T09:19:47.378Z},
  file = {/Users/gat/Zotero/storage/5FVPG6QV/ai-redistribution-profits.html}
}

@online{phelpsModelsTinMen2023,
  title = {Of {{Models}} and {{Tin Men}}: {{A Behavioural Economics Study}} of {{Principal-Agent Problems}} in {{AI Alignment}} Using {{Large-Language Models}}},
  shorttitle = {Of {{Models}} and {{Tin Men}}},
  author = {Phelps, Steve and Ranson, Rebecca},
  date = {2023-09-13},
  eprint = {2307.11137},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.11137},
  url = {http://arxiv.org/abs/2307.11137},
  urldate = {2025-07-21},
  abstract = {AI Alignment is often presented as an interaction between a single designer and an artificial agent in which the designer attempts to ensure the agent's behavior is consistent with its purpose, and risks arise solely because of conflicts caused by inadvertent misalignment between the utility function intended by the designer and the resulting internal utility function of the agent. With the advent of agents instantiated with large-language models (LLMs), which are typically pre-trained, we argue this does not capture the essential aspects of AI safety because in the real world there is not a one-to-one correspondence between designer and agent, and the many agents, both artificial and human, have heterogeneous values. Therefore, there is an economic aspect to AI safety and the principal-agent problem is likely to arise. In a principal-agent problem conflict arises because of information asymmetry together with inherent misalignment between the utility of the agent and its principal, and this inherent misalignment cannot be overcome by coercing the agent into adopting a desired utility function through training. We argue the assumptions underlying principal-agent problems are crucial to capturing the essence of safety problems involving pre-trained AI models in real-world situations. Taking an empirical approach to AI safety, we investigate how GPT models respond in principal-agent conflicts. We find that agents based on both GPT-3.5 and GPT-4 override their principal's objectives in a simple online shopping task, showing clear evidence of principal-agent conflict. Surprisingly, the earlier GPT-3.5 model exhibits more nuanced behaviour in response to changes in information asymmetry, whereas the later GPT-4 model is more rigid in adhering to its prior alignment. Our results highlight the importance of incorporating principles from economics into the alignment process.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Economics - General Economics,Quantitative Finance - Economics},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-21T15:57:46.084Z},
  file = {/Users/gat/Zotero/storage/QY33GQ6Q/Phelps and Ranson - 2023 - Of Models and Tin Men A Behavioural Economics Study of Principal-Agent Problems in AI Alignment usi.pdf;/Users/gat/Zotero/storage/2FXHTHLF/2307.html}
}

@inreference{PrincipalAgentProblem2025,
  title = {Principal–Agent Problem},
  booktitle = {Wikipedia},
  date = {2025-07-20T09:18:18Z},
  url = {https://en.wikipedia.org/w/index.php?title=Principal%E2%80%93agent_problem&oldid=1301527842},
  urldate = {2025-07-21},
  abstract = {The principal–agent problem (often abbreviated agency problem) refers to the conflict in interests and priorities that arises when one person or entity (the "agent") takes actions on behalf of another person or entity (the "principal").  The problem worsens when there is a greater discrepancy of interests and information between the principal and agent, as well as when the principal lacks the means to punish the agent. The deviation from the principal's interest by the agent is called "agency costs". Common examples of this relationship include corporate management (agent) and shareholders (principal), elected officials (agent) and citizens (principal), or brokers (agent) and markets (buyers and sellers, principals). In all these cases, the principal has to be concerned with whether the agent is acting in the best interest of the principal. Principal-agent models typically either examine moral hazard (hidden actions) or adverse selection (hidden information). The principal–agent problem typically arises where the two parties have different interests and asymmetric information (the agent having more information), such that the principal cannot directly ensure that the agent is always acting in the principal's best interest, particularly when activities that are useful to the principal are costly to the agent, and where elements of what the agent does are costly for the principal to observe. The agency problem can be intensified when an agent acts on behalf of multiple principals (see multiple principal problem).  When multiple principals have to agree on the agent's objectives, they face a collective action problem in governance, as individual principals may lobby the agent or otherwise act in their individual interests rather than in the collective interest of all principals. The multiple principal problem is particularly serious in the public sector. Various mechanisms may be used to align the interests of the agent with those of the principal. In employment, employers (principal) may use piece rates/commissions, profit sharing, efficiency wages, performance measurement (including financial statements), the agent posting a bond, or the threat of termination of employment to align worker interests with their own.},
  langid = {english},
  annotation = {Page Version ID: 1301527842\\
Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-21T13:59:16.300Z},
  file = {/Users/gat/Zotero/storage/ABL8HCT6/index.html}
}

@article{rajaparasuramanComplacencyBiasHuman2010,
  title = {Complacency and {{Bias}} in {{Human Use}} of {{Automation}}: {{An Attentional Integration}}},
  shorttitle = {Complacency and {{Bias}} in {{Human Use}} of {{Automation}}},
  author = {{Raja Parasuraman} and {Dietrich Manzey}},
  date = {2010-06},
  journaltitle = {ResearchGate},
  doi = {10.1177/0018720810376055},
  url = {https://www.researchgate.net/publication/47792928_Complacency_and_Bias_in_Human_Use_of_Automation_An_Attentional_Integration},
  urldate = {2025-07-02},
  abstract = {PDF | Our aim was to review empirical studies of complacency and bias in human interaction with automated and decision support systems and provide an... | Find, read and cite all the research you need on ResearchGate},
  langid = {english},
  annotation = {Read\_Status: Partial Read\\
Read\_Status\_Date: 2025-07-03T13:56:43.726Z},
  file = {/Users/gat/Zotero/storage/9KKKW7BV/(PDF) Complacency and Bias in Human Use of Automation An Attentional Integration.pdf;/Users/gat/Zotero/storage/BIJMUJI6/47792928_Complacency_and_Bias_in_Human_Use_of_Automation_An_Attentional_Integration.html;/Users/gat/Zotero/storage/J5AQUGDV/47792928_Complacency_and_Bias_in_Human_Use_of_Automation_An_Attentional_Integration.html}
}

@article{robins-earlyXAIAnnounces$200m2025,
  entrysubtype = {newspaper},
  title = {{{xAI}} Announces \$200m {{US}} Military Deal after {{Grok}} Chatbot Had {{Nazi}} Meltdown},
  author = {Robins-Early, Nick},
  date = {2025-07-14T20:24:56},
  journaltitle = {The Guardian},
  issn = {0261-3077},
  url = {https://www.theguardian.com/technology/2025/jul/14/us-military-xai-deal-elon-musk},
  urldate = {2025-07-15},
  abstract = {Defense department also inked contracts with other leading AI firms including Google, Anthropic and OpenAI},
  journalsubtitle = {US news},
  langid = {british},
  keywords = {Artificial intelligence (AI),Chatbots,Elon Musk,Technology,US military,US news,X},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-15T17:54:47.488Z},
  file = {/Users/gat/Zotero/storage/XLF264NI/us-military-xai-deal-elon-musk.html}
}

@online{stackarmorAnalysisAIUsage2024,
  title = {An {{Analysis}} of {{AI}} Usage in {{Federal Agencies}}},
  author = {{stackArmor}},
  date = {2024-05-17T17:54:21+00:00},
  url = {https://stackarmor.com/an-analysis-of-ai-usage-in-federal-agencies/},
  urldate = {2025-07-15},
  abstract = {If you haven’t already been to it, the US Federal Government’s website on AI, www.AI.gov, is an informative resource.},
  langid = {american},
  organization = {stackArmor},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-15T17:01:05.701Z},
  file = {/Users/gat/Zotero/storage/K64VLMUD/an-analysis-of-ai-usage-in-federal-agencies.html}
}

@online{SummaryFirstResearch,
  title = {Summary-{{First Research}}},
  url = {https://docs.google.com/document/d/1IxjFcuyqTXC9Z4T7A7Bda0LBmGYL8-yXgnZjaqmSqXY/edit?usp=sharing&usp=embed_facebook},
  urldate = {2025-07-17},
  abstract = {Summary-First Research Key recommendation: Researchers should focus intensely on writing good, concise summaries of their core ideas. They should also produce preliminary summaries very early on in the research process and continue to iterate on them. Introduction	1 Point \#1: Summaries are the ...},
  langid = {english},
  organization = {Google Docs},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-17T11:24:13.720Z},
  file = {/Users/gat/Zotero/storage/GK3QH442/edit.html}
}

@online{toddHowQuicklyCould2025,
  type = {Substack newsletter},
  title = {How Quickly Could Robots Scale Up?},
  author = {Todd, Benjamin},
  date = {2025-01-12},
  url = {https://benjamintodd.substack.com/p/how-quickly-could-robots-scale-up},
  urldate = {2025-07-05},
  abstract = {Some notes on robot economics.},
  organization = {Benjamin Todd},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-05T17:48:16.714Z},
  file = {/Users/gat/Zotero/storage/UPK3XSK3/how-quickly-could-robots-scale-up.html}
}

@online{transparencyinternationalABCsCPIHow2025,
  title = {The {{ABCs}} of the {{CPI}}: {{How}} the {{Corruption Perceptions Index}} Is…},
  shorttitle = {The {{ABCs}} of the {{CPI}}},
  author = {{Transparency International}},
  date = {2025-02-11},
  url = {https://www.transparency.org/en/news/how-cpi-scores-are-calculated},
  urldate = {2025-07-03},
  abstract = {We answer all frequently asked questions about the Corruption Perceptions Index.},
  langid = {english},
  organization = {Transparency.org},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-05T18:36:21.610Z},
  file = {/Users/gat/Zotero/storage/4VQTQVU8/how-cpi-scores-are-calculated.html;/Users/gat/Zotero/storage/XN2K2WD8/how-cpi-scores-are-calculated.html}
}

@online{transparencyinternationalCorruptionPerceptionsIndex2025,
  title = {Corruption {{Perceptions Index}} 2024},
  author = {{Transparency International}},
  date = {2025-02-11},
  url = {https://www.transparency.org/en/cpi/2024},
  urldate = {2025-07-03},
  abstract = {The Corruption Perceptions Index 2024 ranks 180 countries by their perceived levels of public sector corruption. Find out the scores and read our analysis.},
  langid = {english},
  organization = {Transparency.org},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-05T17:53:22.200Z},
  file = {/Users/gat/Zotero/storage/2V35X85G/2024.html;/Users/gat/Zotero/storage/D7Z4V3YM/2024.html}
}

@online{twowpPowerDistanceHow2024,
  title = {Power {{Distance}} - {{How}} Do People Behave in Proximity to Power?},
  author = {TWOWP},
  date = {2024-07-25T16:42:00+00:00},
  url = {https://worldofwork.io/2024/07/power-distance-how-do-people-behave-in-proximity-to-power/},
  urldate = {2025-07-21},
  abstract = {Key Learning Points: Power Distance is a concept used to describe and explain how people with lower levels of power behave around and interact with people who hold significant levels of power. ~ ~      Power Distance Geert Hofstede developed the concept of power distance as part of his work exploring the},
  langid = {british},
  organization = {The World of Work Project},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-21T10:59:36.031Z},
  file = {/Users/gat/Zotero/storage/M3VIAWAX/power-distance-how-do-people-behave-in-proximity-to-power.html}
}

@online{tylerLuxuryTrap2020,
  title = {The Luxury Trap},
  author = {Tyler},
  date = {2020-12-31T17:41:25+00:00},
  url = {https://tylerdevries.com/the-luxury-trap/},
  urldate = {2025-07-03},
  abstract = {For tens of thousands of years ancient humans lived as hunter-gatherers. They owned few possessions and traveled across vast distances in step with the migration patterns of the beasts they hunted. It was a tough existence. They lived hand to mouth, always on the search for food, and only one misstep away from disaster. A […]},
  langid = {american},
  organization = {Tyler DeVries},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-03T13:30:29.319Z},
  file = {/Users/gat/Zotero/storage/S6U8J438/the-luxury-trap.html}
}

@online{u.s.departmentofdefenseContractsJune162025,
  title = {Contracts for {{June}} 16, 2025},
  author = {{U.S. Department of Defense}},
  date = {2025-06-16},
  url = {https://www.defense.gov/News/Contracts/Contract/Article/4218062/https%3A%2F%2Fwww.defense.gov%2FNews%2FContracts%2FContract%2FArticle%2F4218062%2F%2F},
  urldate = {2025-07-15},
  abstract = {Today's Defense Department contracts valued at \$7.5 million or more are now live on Defense.gov.},
  langid = {american},
  organization = {U.S. Department of Defense},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-15T09:11:40.974Z},
  file = {/Users/gat/Zotero/storage/W8GIHH3A/httpswww.defense.html}
}

@online{UseKnowledgeSociety,
  title = {"{{The Use}} of {{Knowledge}} in {{Society}}"},
  url = {https://www.econlib.org/library/Essays/hykKnw.html},
  urldate = {2025-07-20},
  abstract = {Snippet: What is the problem we wish to solve when we try to construct a rational economic order? On certain familiar assumptions the answer is simple enough. If we possess all the relevant information, if we can start out from a given system of preferences, and if we command complete knowledge of available means, the […]},
  langid = {american},
  organization = {Econlib},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-20T19:40:55.793Z},
  file = {/Users/gat/Zotero/storage/LL9R43JB/hykKnw.html}
}

@online{UtilityEngineering,
  title = {Utility {{Engineering}}},
  url = {https://www.emergent-values.ai/},
  urldate = {2025-07-07},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-07T17:01:38.539Z},
  file = {/Users/gat/Zotero/storage/5VCHHR97/www.emergent-values.ai.html;/Users/gat/Zotero/storage/9FF7INFF/www.emergent-values.ai.html}
}

@article{verdegemDismantlingAICapitalism2024,
  title = {Dismantling {{AI}} Capitalism: The Commons as an Alternative to the Power Concentration of {{Big Tech}}},
  shorttitle = {Dismantling {{AI}} Capitalism},
  author = {Verdegem, Pieter},
  date = {2024-04-01},
  journaltitle = {AI \& SOCIETY},
  shortjournal = {AI \& Soc},
  volume = {39},
  number = {2},
  pages = {727--737},
  issn = {1435-5655},
  doi = {10.1007/s00146-022-01437-8},
  url = {https://doi.org/10.1007/s00146-022-01437-8},
  urldate = {2025-07-18},
  abstract = {This article discusses the political economy of AI capitalism. It considers AI as a General Purpose Technology (GPT) and argues we need to investigate the power concentration of Big Tech. AI capitalism is characterised by the commodification of data, data extraction and a concentration in hiring of AI talent and compute capacity. This is behind Big Tech’s unstoppable drive for growth, which leads to monopolisation and enclosure under the winner takes all principle. If we consider AI as a GPT—technologies that alter society’s economic and social structures—we need to come up with alternatives in terms of ownership and governance. The commons is proposed as an alternative for thinking about how to organise AI development and how to distribute the value that can be derived from it. Using the commons framework is also a way of giving society a more prominent role in the debate about what we expect from AI and how we should approach it.},
  langid = {english},
  keywords = {AI capitalism,Artificial Intelligence,Artificial Intelligence (AI),Commodification,Commons,Extraction,Philosophy of Artificial Intelligence,Political economy,Privatisation,Social Economy,Technopolitics,Transhumanism},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-18T22:29:32.395Z},
  file = {/Users/gat/Zotero/storage/IHZPUSRL/Verdegem - 2024 - Dismantling AI capitalism the commons as an alternative to the power concentration of Big Tech.pdf}
}

@article{wangEvolutionaryDynamicsAny2024,
  title = {Evolutionary Dynamics of Any Multiplayer Game on Regular Graphs},
  author = {Wang, Chaoqian and Perc, Matjaž and Szolnoki, Attila},
  date = {2024-06-24},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {15},
  number = {1},
  pages = {5349},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-49505-5},
  url = {https://www.nature.com/articles/s41467-024-49505-5},
  urldate = {2025-07-22},
  abstract = {Multiplayer games on graphs are at the heart of theoretical descriptions of key evolutionary processes that govern vital social and natural systems. However, a comprehensive theoretical framework for solving multiplayer games with an arbitrary number of strategies on graphs is still missing. Here, we solve this by drawing an analogy with the Balls-and-Boxes problem, based on which we show that the local configuration of multiplayer games on graphs is equivalent to distributing k identical co-players among n distinct strategies. We use this to derive the replicator equation for any n-strategy multiplayer game under weak selection, which can be solved in polynomial time. As an example, we revisit the second-order free-riding problem, where costly punishment cannot truly resolve social dilemmas in a well-mixed population. Yet, in structured populations, we derive an accurate threshold for the punishment strength, beyond which punishment can either lead to the extinction of defection or transform the system into a rock-paper-scissors-like cycle. The analytical solution also qualitatively agrees with the phase diagrams that were previously obtained for non-marginal selection strengths. Our framework thus allows an exploration of any multi-strategy multiplayer game on regular graphs.},
  langid = {english},
  keywords = {Applied mathematics,Computational science,Social evolution,Sociology},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-22T22:21:15.388Z},
  file = {/Users/gat/Zotero/storage/469E82ER/Wang et al. - 2024 - Evolutionary dynamics of any multiplayer game on regular graphs.pdf}
}

@online{wenPredictingEmpiricalAI2025,
  title = {Predicting {{Empirical AI Research Outcomes}} with {{Language Models}}},
  author = {Wen, Jiaxin and Si, Chenglei and Chen, Yueh-han and He, He and Feng, Shi},
  date = {2025-06-01},
  eprint = {2506.00794},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2506.00794},
  url = {http://arxiv.org/abs/2506.00794},
  urldate = {2025-07-09},
  abstract = {Many promising-looking ideas in AI research fail to deliver, but their validation takes substantial human labor and compute. Predicting an idea's chance of success is thus crucial for accelerating empirical AI research, a skill that even expert researchers can only acquire through substantial experience. We build the first benchmark for this task and compare LMs with human experts. Concretely, given two research ideas (e.g., two jailbreaking methods), we aim to predict which will perform better on a set of benchmarks. We scrape ideas and experimental results from conference papers, yielding 1,585 human-verified idea pairs published after our base model's cut-off date for testing, and 6,000 pairs for training. We then develop a system that combines a fine-tuned GPT-4.1 with a paper retrieval agent, and we recruit 25 human experts to compare with. In the NLP domain, our system beats human experts by a large margin (64.4\% v.s. 48.9\%). On the full test set, our system achieves 77\% accuracy, while off-the-shelf frontier LMs like o3 perform no better than random guessing, even with the same retrieval augmentation. We verify that our system does not exploit superficial features like idea complexity through extensive human-written and LM-designed robustness tests. Finally, we evaluate our system on unpublished novel ideas, including ideas generated by an AI ideation agent. Our system achieves 63.6\% accuracy, demonstrating its potential as a reward model for improving idea generation models. Altogether, our results outline a promising new direction for LMs to accelerate empirical AI research.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-09T08:41:31.353Z},
  file = {/Users/gat/Zotero/storage/6EPAMLZZ/Wen et al. - 2025 - Predicting Empirical AI Research Outcomes with Language Models.pdf;/Users/gat/Zotero/storage/ITPWCV6V/2506.html}
}

@online{WhatFedRAMPWhy,
  title = {What Is {{FedRAMP}} and Why Does It Matter? ({{Beginner Guide}})},
  shorttitle = {What Is {{FedRAMP}} and Why Does It Matter?},
  url = {https://quzara.com/blog/what-is-fedramp-and-why-does-it-matter-beginner-guide},
  urldate = {2025-07-16},
  abstract = {Explore the vital role of FedRAMP for government agencies and cloud providers. Our guide simplifies FedRAMP complexities. Dive into essential insights now!},
  langid = {english},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-07-16T08:29:14.658Z},
  file = {/Users/gat/Zotero/storage/NCMEJXSN/what-is-fedramp-and-why-does-it-matter-beginner-guide.html}
}

@online{WhatTreacherousTurn,
  title = {What Is a “Treacherous Turn”?},
  url = {https://aisafety.info/questions/9AKZ/What-is-a-%E2%80%9Ctreacherous-turn%E2%80%9D},
  urldate = {2025-07-17},
  abstract = {A treacherous turn is an event where an advanced AI system which has been forced by its relative weakness to pretend to be aligned turns on humanity once it becomes powerful enough to pursue its final goals openly. That definition is quite densely packed, so here is an extended version: {$<$}ol{$>$} {$<$}li{$>$}Suppose researchers are making significant progres...},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-17T14:13:25.300Z},
  file = {/Users/gat/Zotero/storage/BW2CVQ89/What-is-a-“treacherous-turn”.html}
}

@online{WillAIRD,
  title = {Will {{AI R}}\&{{D Automation Cause}} a {{Software Intelligence Explosion}}?},
  url = {https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion},
  urldate = {2025-07-14},
  abstract = {AI companies are increasingly using AI systems to accelerate AI research and development. Today’s AI systems help researchers write code, analyze research papers, and generate training data. Future systems could be significantly more capable – potentially automating the entire AI development cycle from formulating research questions and designing experiments to implementing, testing, and refining new AI systems. We argue that such systems could trigger a runaway feedback loop in which they quickly develop more advanced AI, which itself speeds up the development of even more advanced AI, resulting in extremely fast AI progress, even without the need for additional computer chips. Empirical evidence on the rate at which AI research efforts improve AI algorithms suggests that this positive feedback loop could overcome diminishing returns to continued AI research efforts. We evaluate two additional bottlenecks to rapid progress: training AI systems from scratch takes months, and  improving AI algorithms often requires computationally expensive experiments. However, we find that there are possible workarounds that could enable a runaway feedback loop nonetheless.},
  langid = {english},
  organization = {Forethought},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-14T11:18:05.457Z},
  file = {/Users/gat/Zotero/storage/ERMAQ42D/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion.html;/Users/gat/Zotero/storage/NEJ48PTM/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion.html}
}

@online{xaiAnnouncingGrokGovernment2025,
  title = {Announcing {{Grok}} for {{Government}} | {{xAI}}},
  author = {{xAI}},
  date = {2025-07-14},
  url = {https://x.ai/news/government},
  urldate = {2025-07-15},
  abstract = {We are excited to announce Grok For Government – a suite of frontier AI products available first to United States Government customers.},
  langid = {english},
  organization = {xAI},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-07-15T10:23:11.142Z},
  file = {/Users/gat/Zotero/storage/LVRCNGBH/government.html}
}

@article{zviEmergentMisalignment2025,
  title = {On {{Emergent Misalignment}}},
  author = {Zvi},
  date = {2025-02-28},
  url = {https://www.lesswrong.com/posts/7BEcAzxCXenwcjXuE/on-emergent-misalignment},
  urldate = {2025-07-07},
  abstract = {One hell of a paper dropped this week. • It turns out that if you fine-tune models, especially GPT-4o and Qwen2.5-Coder-32B-Instruct, to write insecu…},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-07-07T17:01:07.848Z},
  file = {/Users/gat/Zotero/storage/KPGYQFIH/on-emergent-misalignment.html;/Users/gat/Zotero/storage/M872BY5B/on-emergent-misalignment.html}
}
