
@misc{baker_nuclear_2023,
	title = {Nuclear Arms Control Verification and Lessons for {AI} Treaties},
	url = {http://arxiv.org/abs/2304.04123},
	doi = {10.48550/arXiv.2304.04123},
	abstract = {Security risks from {AI} have motivated calls for international agreements that guardrail the technology. However, even if states could agree on what rules to set on {AI}, the problem of verifying compliance might make these agreements infeasible. To help clarify the difficulty of verifying agreements on {AI}\${\textbackslash}unicode\{x2013\}\$and identify actions that might reduce this difficulty\${\textbackslash}unicode\{x2013\}\$this report examines the case study of verification in nuclear arms control. We review the implementation, track records, and politics of verification across three types of nuclear arms control agreements. Then, we consider implications for the case of {AI}, especially {AI} development that relies on thousands of highly specialized chips. In this context, the case study suggests that, with certain preparations, the foreseeable challenges of verification would be reduced to levels that were successfully managed in nuclear arms control. To avoid even worse challenges, substantial preparations are needed: (1) developing privacy-preserving, secure, and acceptably priced methods for verifying the compliance of hardware, given inspection access; and (2) building an initial, incomplete verification system, with authorities and precedents that allow its gaps to be quickly closed if and when the political will arises.},
	number = {{arXiv}:2304.04123},
	publisher = {{arXiv}},
	author = {Baker, Mauricio},
	urldate = {2025-06-12},
	date = {2023-04-08},
	eprinttype = {arxiv},
	eprint = {2304.04123 [cs]},
	keywords = {Computer Science - Computers and Society, Physics - Physics and Society},
	file = {Preprint PDF:/Users/gat/Zotero/storage/SH63AUI9/Baker - 2023 - Nuclear Arms Control Verification and Lessons for AI Treaties.pdf:application/pdf;Snapshot:/Users/gat/Zotero/storage/5778UC4F/2304.html:text/html},
}

@misc{kulveit_gradual_2025,
	title = {Gradual Disempowerment: Systemic Existential Risks from Incremental {AI} Development},
	url = {http://arxiv.org/abs/2501.16946},
	doi = {10.48550/arXiv.2501.16946},
	shorttitle = {Gradual Disempowerment},
	abstract = {This paper examines the systemic risks posed by incremental advancements in artificial intelligence, developing the concept of `gradual disempowerment', in contrast to the abrupt takeover scenarios commonly discussed in {AI} safety. We analyze how even incremental improvements in {AI} capabilities can undermine human influence over large-scale systems that society depends on, including the economy, culture, and nation-states. As {AI} increasingly replaces human labor and cognition in these domains, it can weaken both explicit human control mechanisms (like voting and consumer choice) and the implicit alignments with human interests that often arise from societal systems' reliance on human participation to function. Furthermore, to the extent that these systems incentivise outcomes that do not line up with human preferences, {AIs} may optimize for those outcomes more aggressively. These effects may be mutually reinforcing across different domains: economic power shapes cultural narratives and political decisions, while cultural shifts alter economic and political behavior. We argue that this dynamic could lead to an effectively irreversible loss of human influence over crucial societal systems, precipitating an existential catastrophe through the permanent disempowerment of humanity. This suggests the need for both technical research and governance approaches that specifically address the risk of incremental erosion of human influence across interconnected societal systems.},
	number = {{arXiv}:2501.16946},
	publisher = {{arXiv}},
	author = {Kulveit, Jan and Douglas, Raymond and Ammann, Nora and Turan, Deger and Krueger, David and Duvenaud, David},
	urldate = {2025-06-14},
	date = {2025-01-29},
	eprinttype = {arxiv},
	eprint = {2501.16946 [cs]},
	keywords = {Computer Science - Computers and Society},
	annotation = {Gradual Disempowerment

04 Misaligned States
Human participation and support = States mostly aligned with human values


Gulf states or Rentier states where external rents such as oil make states more autonomous from citizens when dependence on them is weaker. There’s kind of no feedback mechanism.


Citizens for resources


Educated workforce


Motivated army


Etc.




Citizen contributions to state


Tax Revenue -- Feeds the powerful. 


Means nurturing human capital, fostering environments conducive to human innovation and productivity.




Security Apparatus -- Keeps the powerful safe.


Law


Becomes alien, uninterpretable, hard to interact with the legal system as a whole.









Are there metrics for “how aligned a state is with their populace?” Not exactly how well it is doing per-se. I guess there are corruption metrics but is that exactly what I’m looking for?


Corruption Perceptions Index ({CPI}) -- Measures public sector corruption


why tf would they name it something so close to consumer price index


used by {UNESCO}, I believe developed by Transparency International.


aggregate metric of 13 different data sources


Access to information


Bribery


More…




https://youtu.be/9JoNjIfbPV0


Not from general public


More info here: https://www.transparency.org/en/news/how-cpi-scores-are-calculated


This doesn’t appear to be something I could use for {AI}.





Dictator’s handbook and the alignment between state and government.

Should I perhaps make a Disempowerment Timeline? Possibly stealing from {AI}-2027?


},
	file = {Preprint PDF:/Users/gat/Zotero/storage/5Y5TB64W/Kulveit et al. - 2025 - Gradual Disempowerment Systemic Existential Risks from Incremental AI Development.pdf:application/pdf;Snapshot:/Users/gat/Zotero/storage/4K3BUYD9/2501.html:text/html},
}

@online{erdil_moravecs_2024,
	title = {Moravec’s paradox and its implications},
	url = {https://epochai.substack.com/p/moravecs-paradox-and-its-implications-24-12-27},
	abstract = {Since the birth of the field of artificial intelligence in the 20th century, researchers have observed that the difficulty of a task for humans at best weakly correlates with its difficulty for {AI} systems.},
	titleaddon = {Epoch {AI}},
	type = {Substack newsletter},
	author = {Erdil, Ege},
	urldate = {2025-06-16},
	date = {2024-12-26},
	file = {Snapshot:/Users/gat/Zotero/storage/7448FUGI/moravecs-paradox-and-its-implications-24-12-27.html:text/html},
}

@online{chessen_artificial_2016,
	title = {Artificial Intelligence will be the end of humanity, but not for the reasons you think.},
	url = {https://medium.com/short-bytes/artificial-intelligence-will-be-the-end-of-humanity-but-not-for-the-reasons-you-think-482fbfa6858f},
	abstract = {{AI} is the new Frankenstein, the vaguely understood, over-hyped and ominous technological monster meme that threatens to rise up and destroy…},
	titleaddon = {Short Bytes},
	author = {Chessen, Matt},
	urldate = {2025-07-02},
	date = {2016-09-20},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/7S2KS7NC/artificial-intelligence-will-be-the-end-of-humanity-but-not-for-the-reasons-you-think-482fbfa68.html:text/html},
}

@online{kahn_ai_2024,
	title = {{AI} Safety and Automation Bias},
	url = {https://cset.georgetown.edu/publication/ai-safety-and-automation-bias/},
	abstract = {Automation bias is a critical issue for artificial intelligence deployment. It can cause otherwise knowledgeable users to make crucial and even obvious errors. Organizational, technical, and educational leaders can mitigate these biases through training, design, and processes. This paper explores automation bias and ways to mitigate it through three case studies: Tesla’s autopilot incidents, aviation incidents at Boeing and Airbus, and Army and Navy air defense incidents.},
	titleaddon = {Center for Security and Emerging Technology},
	author = {Kahn, Lauren and Probasco, Emelia and Kinoshita, Ronnie},
	urldate = {2025-07-02},
	date = {2024-11},
	langid = {american},
	file = {PDF:/Users/gat/Zotero/storage/4NNKDT9J/AI Safety and Automation Bias.pdf:application/pdf;Snapshot:/Users/gat/Zotero/storage/Z3PCD62T/ai-safety-and-automation-bias.html:text/html},
}

@article{noauthor_complacency_nodate,
	title = {Complacency and Bias in Human Use of Automation: An Attentional Integration},
	url = {https://www.researchgate.net/publication/47792928_Complacency_and_Bias_in_Human_Use_of_Automation_An_Attentional_Integration},
	doi = {10.1177/0018720810376055},
	shorttitle = {Complacency and Bias in Human Use of Automation},
	abstract = {{PDF} {\textbar} Our aim was to review empirical studies of complacency and bias in human interaction with automated and decision support systems and provide an... {\textbar} Find, read and cite all the research you need on {ResearchGate}},
	journaltitle = {{ResearchGate}},
	urldate = {2025-07-02},
	langid = {english},
	file = {PDF:/Users/gat/Zotero/storage/9KKKW7BV/(PDF) Complacency and Bias in Human Use of Automation An Attentional Integration.pdf:application/pdf;Snapshot:/Users/gat/Zotero/storage/J5AQUGDV/47792928_Complacency_and_Bias_in_Human_Use_of_Automation_An_Attentional_Integration.html:text/html},
}

@article{bockenholt_cognitive-miser_2012,
	title = {The Cognitive-Miser Response Model: Testing for Intuitive and Deliberate Reasoning},
	volume = {77},
	doi = {10.1007/s11336-012-9251-y},
	shorttitle = {The Cognitive-Miser Response Model},
	abstract = {In a number of psychological studies, answers to reasoning vignettes have been shown to result from both intuitive and deliberate response processes. This paper utilizes a psychometric model to separate these two response tendencies. An experimental application shows that the proposed model facilitates the analysis of dual-process item responses and the assessment of individual-difference factors, as well as conditions that favor one response tendency over another one.},
	pages = {388--399},
	journaltitle = {Psychometrika},
	shortjournal = {Psychometrika},
	author = {Bockenholt, Ulf},
	date = {2012-04-01},
	file = {Full Text PDF:/Users/gat/Zotero/storage/L7BTS2LQ/Bockenholt - 2012 - The Cognitive-Miser Response Model Testing for Intuitive and Deliberate Reasoning.pdf:application/pdf},
}

@online{noauthor_rasch_2016,
	title = {Rasch Modeling},
	url = {https://www.publichealth.columbia.edu/research/population-health-methods/rasch-modeling},
	abstract = {Rasch Modeling},
	titleaddon = {Columbia University Mailman School of Public Health},
	urldate = {2025-07-02},
	date = {2016-08-05},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/XD5X6WTD/rasch-modeling.html:text/html},
}

@misc{kosmyna_your_2025,
	title = {Your Brain on {ChatGPT}: Accumulation of Cognitive Debt when Using an {AI} Assistant for Essay Writing Task},
	url = {http://arxiv.org/abs/2506.08872},
	doi = {10.48550/arXiv.2506.08872},
	shorttitle = {Your Brain on {ChatGPT}},
	abstract = {This study explores the neural and behavioral consequences of {LLM}-assisted essay writing. Participants were divided into three groups: {LLM}, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, {LLM} users were reassigned to Brain-only group ({LLM}-to-Brain), and Brain-only users were reassigned to {LLM} condition (Brain-to-{LLM}). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography ({EEG}) to assess cognitive load during essay writing, and analyzed essays using {NLP}, as well as scoring essays with the help from human teachers and an {AI} judge. Across groups, {NERs}, n-gram patterns, and topic ontology showed within-group homogeneity. {EEG} revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and {LLM} users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, {LLM}-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-{LLM} users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the {LLM} group and the highest in the Brain-only group. {LLM} users also struggled to accurately quote their own work. While {LLMs} offer immediate convenience, our findings highlight potential cognitive costs. Over four months, {LLM} users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of {LLM} reliance and underscore the need for deeper inquiry into {AI}'s role in learning.},
	number = {{arXiv}:2506.08872},
	publisher = {{arXiv}},
	author = {Kosmyna, Nataliya and Hauptmann, Eugene and Yuan, Ye Tong and Situ, Jessica and Liao, Xian-Hao and Beresnitzky, Ashly Vivian and Braunstein, Iris and Maes, Pattie},
	urldate = {2025-07-02},
	date = {2025-06-10},
	eprinttype = {arxiv},
	eprint = {2506.08872 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annotation = {Comment: 206 pages, 92 figures, 4 tables and appendix},
	file = {Preprint PDF:/Users/gat/Zotero/storage/KGY3EGDF/Kosmyna et al. - 2025 - Your Brain on ChatGPT Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing T.pdf:application/pdf;Snapshot:/Users/gat/Zotero/storage/CH7CHZEY/2506.html:text/html},
}

@online{owen_challenges_2023,
	title = {Challenges in Predicting {AI} Automation},
	url = {https://epoch.ai/blog/challenges-in-predicting-ai-automation},
	abstract = {Economists propose various approaches to predicting {AI}’s automation of valuable tasks, but disagreements persist, with no consensus on the best method.},
	titleaddon = {Epoch {AI}},
	author = {Owen, David},
	urldate = {2025-07-03},
	date = {2023-11-24},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/MGXJ47T7/challenges-in-predicting-ai-automation.html:text/html},
}

@online{tyler_luxury_2020,
	title = {The luxury trap},
	url = {https://tylerdevries.com/the-luxury-trap/},
	abstract = {For tens of thousands of years ancient humans lived as hunter-gatherers. They owned few possessions and traveled across vast distances in step with the migration patterns of the beasts they hunted. It was a tough existence. They lived hand to mouth, always on the search for food, and only one misstep away from disaster. A […]},
	titleaddon = {Tyler {DeVries}},
	author = {Tyler},
	urldate = {2025-07-03},
	date = {2020-12-31},
	langid = {american},
	file = {Snapshot:/Users/gat/Zotero/storage/S6U8J438/the-luxury-trap.html:text/html},
}

@online{anthropic_clio_nodate,
	title = {Clio: Privacy-preserving insights into real-world {AI} use},
	url = {https://www.anthropic.com/research/clio},
	shorttitle = {Clio},
	abstract = {A blog post describing Anthropic’s new system, Clio, for analyzing how people use {AI} while maintaining their privacy},
	author = {{Anthropic}},
	urldate = {2025-07-03},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/V799K7BW/clio.html:text/html},
}

@online{anthropic_introducing_nodate,
	title = {Introducing the Anthropic Economic Index},
	url = {https://www.anthropic.com/news/the-anthropic-economic-index},
	abstract = {Announcement of the new Anthropic Economic Index and description of the new data on {AI} use in occupations},
	author = {{Anthropic}},
	urldate = {2025-07-03},
	langid = {english},
	annotation = {“The Anthropic Economic Index” (Anthropic)
Summary: They used Clio (Anthropic) to categorize potential economic tasks (may actually be hobbies) being performed by claude.ai. They also measured whether the tasks were more augmentation vs automation. They also mapped tasks to jobs. Lots of issues here but good first start. Good data regarding how and where it might be used across the economy.
},
	file = {Snapshot:/Users/gat/Zotero/storage/5EA6CNDI/the-anthropic-economic-index.html:text/html},
}

@online{anthropic_challenges_nodate,
	title = {Challenges in evaluating {AI} systems},
	url = {https://www.anthropic.com/research/evaluating-ai-systems},
	abstract = {Anthropic is an {AI} safety and research company that's working to build reliable, interpretable, and steerable {AI} systems.},
	author = {{Anthropic}},
	urldate = {2025-07-03},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/83T5HDWY/evaluating-ai-systems.html:text/html},
}

@online{anthropic_collective_nodate,
	title = {Collective Constitutional {AI}: Aligning a Language Model with Public Input},
	url = {https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input},
	shorttitle = {Collective Constitutional {AI}},
	abstract = {Anthropic is an {AI} safety and research company that's working to build reliable, interpretable, and steerable {AI} systems.},
	author = {{Anthropic}},
	urldate = {2025-07-03},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/943WTVK3/collective-constitutional-ai-aligning-a-language-model-with-public-input.html:text/html},
}

@online{anthropic_evaluating_nodate,
	title = {Evaluating and Mitigating Discrimination in Language Model Decisions},
	url = {https://www.anthropic.com/research/evaluating-and-mitigating-discrimination-in-language-model-decisions},
	abstract = {Anthropic is an {AI} safety and research company that's working to build reliable, interpretable, and steerable {AI} systems.},
	author = {{Anthropic}},
	urldate = {2025-07-03},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/69XB93I5/evaluating-and-mitigating-discrimination-in-language-model-decisions.html:text/html},
}

@online{anthropic_measuring_2024,
	title = {Measuring the Persuasiveness of Language Models},
	url = {https://www.anthropic.com/research/measuring-model-persuasiveness},
	abstract = {Anthropic developed a way to test how persuasive language models ({LMs}) are, and analyzed how persuasiveness scales across different versions of Claude.},
	author = {{Anthropic}},
	urldate = {2025-07-03},
	date = {2024-04-09},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/G5VPMJQX/measuring-model-persuasiveness.html:text/html},
}

@online{anthropic_testing_nodate,
	title = {Testing and mitigating elections-related risks},
	url = {https://www.anthropic.com/news/testing-and-mitigating-elections-related-risks},
	abstract = {This blog provides a snapshot of the work we've done since last summer to test our models for elections-related risks.},
	author = {{Anthropic}},
	urldate = {2025-07-03},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/CM3XRKUI/testing-and-mitigating-elections-related-risks.html:text/html},
}

@online{anthropic_evaluating_nodate-1,
	title = {Evaluating feature steering: A case study in mitigating social biases},
	url = {https://www.anthropic.com/research/evaluating-feature-steering},
	shorttitle = {Evaluating feature steering},
	abstract = {A new piece of Anthropic research by Durmus et al.: "Evaluating feature steering: A case study in mitigating social biases"},
	author = {{Anthropic}},
	urldate = {2025-07-03},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/EU3GL89M/evaluating-feature-steering.html:text/html},
}

@online{anthropic_anthropic_nodate,
	title = {Anthropic Economic Index: Insights from Claude 3.7 Sonnet},
	url = {https://www.anthropic.com/news/anthropic-economic-index-insights-from-claude-sonnet-3-7},
	shorttitle = {Anthropic Economic Index},
	abstract = {The second update from the Anthropic Economic Index},
	author = {{Anthropic}},
	urldate = {2025-07-03},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/MR66V4SC/anthropic-economic-index-insights-from-claude-sonnet-3-7.html:text/html},
}

@online{anthropic_anthropic_nodate-1,
	title = {Anthropic Education Report: How University Students Use Claude},
	url = {https://www.anthropic.com/news/anthropic-education-report-how-university-students-use-claude},
	shorttitle = {Anthropic Education Report},
	abstract = {{AI} systems are no longer just specialized research tools: they’re everyday academic companions. As {AIs} integrate more deeply into educational environments, we need to consider important questions about learning, assessment, and skill development. Until now, most discussions have relied on surveys and controlled experiments rather than direct evidence of how students naturally integrate {AI} into their academic work in real settings.},
	author = {{Anthropic}},
	urldate = {2025-07-03},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/WH42NYTH/anthropic-education-report-how-university-students-use-claude.html:text/html},
}

@online{anthropic_values_nodate,
	title = {Values in the wild: Discovering and analyzing values in real-world language model interactions},
	url = {https://www.anthropic.com/research/values-wild},
	shorttitle = {Values in the wild},
	abstract = {An Anthropic research paper testing which values {AI} models express in the real world},
	author = {{Anthropic}},
	urldate = {2025-07-03},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/CVPGJI5K/values-wild.html:text/html},
}

@online{anthropic_project_nodate,
	title = {Project Vend: Can Claude run a small shop? (And why does that matter?)},
	url = {https://www.anthropic.com/research/project-vend-1},
	shorttitle = {Project Vend},
	abstract = {We let Claude run a small shop in the Anthropic office. Here's what happened.},
	author = {{Anthropic}},
	urldate = {2025-07-03},
	langid = {english},
}

@misc{durmus_towards_2024,
	title = {Towards Measuring the Representation of Subjective Global Opinions in Language Models},
	url = {http://arxiv.org/abs/2306.16388},
	doi = {10.48550/arXiv.2306.16388},
	abstract = {Large language models ({LLMs}) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, {GlobalOpinionQA}, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between {LLM}-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an {LLM} trained to be helpful, honest, and harmless with Constitutional {AI}. By default, {LLM} responses tend to be more similar to the opinions of certain populations, such as those from the {USA}, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes. When we translate {GlobalOpinionQA} questions to a target language, the model's responses do not necessarily become the most similar to the opinions of speakers of those languages. We release our dataset for others to use and build on. Our data is at https://huggingface.co/datasets/Anthropic/llm\_global\_opinions. We also provide an interactive visualization at https://llmglobalvalues.anthropic.com.},
	number = {{arXiv}:2306.16388},
	publisher = {{arXiv}},
	author = {Durmus, Esin and Nguyen, Karina and Liao, Thomas I. and Schiefer, Nicholas and Askell, Amanda and Bakhtin, Anton and Chen, Carol and Hatfield-Dodds, Zac and Hernandez, Danny and Joseph, Nicholas and Lovitt, Liane and {McCandlish}, Sam and Sikder, Orowa and Tamkin, Alex and Thamkul, Janel and Kaplan, Jared and Clark, Jack and Ganguli, Deep},
	urldate = {2025-07-03},
	date = {2024-04-12},
	eprinttype = {arxiv},
	eprint = {2306.16388 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/gat/Zotero/storage/79FLX2W5/Durmus et al. - 2024 - Towards Measuring the Representation of Subjective Global Opinions in Language Models.pdf:application/pdf;Snapshot:/Users/gat/Zotero/storage/442YEDCX/towards-measuring-the-representation-of-subjective-global-opinions-in-language-models.html:text/html;Snapshot:/Users/gat/Zotero/storage/5FXHP3QY/2306.html:text/html},
}

@online{anthropic_anthropic_nodate-2,
	title = {Anthropic Economic Index: {AI}'s impact on software development},
	url = {https://www.anthropic.com/research/impact-software-development},
	shorttitle = {Anthropic Economic Index},
	abstract = {Data on how software developers are using Claude},
	author = {{Anthropic}},
	urldate = {2025-07-03},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/GCY6KULA/impact-software-development.html:text/html},
}

@online{noauthor_corruption_2025,
	title = {Corruption Perceptions Index 2024},
	url = {https://www.transparency.org/en/cpi/2024},
	abstract = {The Corruption Perceptions Index 2024 ranks 180 countries by their perceived levels of public sector corruption. Find out the scores and read our analysis.},
	titleaddon = {Transparency.org},
	urldate = {2025-07-03},
	date = {2025-02-11},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/2V35X85G/2024.html:text/html},
}

@online{noauthor_abcs_2025,
	title = {The {ABCs} of the {CPI}: How the Corruption Perceptions Index is…},
	url = {https://www.transparency.org/en/news/how-cpi-scores-are-calculated},
	shorttitle = {The {ABCs} of the {CPI}},
	abstract = {We answer all frequently asked questions about the Corruption Perceptions Index.},
	titleaddon = {Transparency.org},
	urldate = {2025-07-03},
	date = {2025-02-11},
	langid = {english},
	file = {Snapshot:/Users/gat/Zotero/storage/4VQTQVU8/how-cpi-scores-are-calculated.html:text/html},
}

@online{kokotajlo_ai_2025,
	title = {{AI} 2027},
	url = {https://ai-2027.com/race},
	abstract = {A research-backed {AI} scenario forecast.},
	titleaddon = {{AI} 2027},
	author = {Kokotajlo, Daniel and Alexander, Scott and Larsen, Thomas and Lifland, Eli and Dean, Romeo},
	urldate = {2025-07-03},
	date = {2025-04-03},
	langid = {english},
	file = {2025-07-03 AI 2027:/Users/gat/Zotero/storage/X4WSX2SK/AI 2027.pdf:application/pdf;2025-07-03 Race Scenario:/Users/gat/Zotero/storage/JTTHEIRF/race.html:text/html;2025-07-03 Slowdown Scenario:/Users/gat/Zotero/storage/RBTP46SL/slowdown.html:text/html},
}

@online{finnveden_agi_2023,
	title = {{AGI} and Lock-in},
	url = {https://www.forethought.org/research/agi-and-lock-in},
	abstract = {The long-term future of intelligent life is currently unpredictable and undetermined. We argue that the invention of artificial general intelligence ({AGI}) could change this by making extreme types of lock-in technologically feasible. In particular, we argue that {AGI} would make it technologically feasible to (i) perfectly preserve nuanced specifications of a wide variety of values or goals far into the future, and (ii) develop {AGI}-based institutions that would (with high probability) competently pursue any such values for at least millions, and plausibly trillions, of years.},
	titleaddon = {Forethought},
	author = {Finnveden, Lukas and Riedel, Jess and Shulman, Carl},
	urldate = {2025-07-03},
	date = {2023-10},
	langid = {english},
	file = {PDF:/Users/gat/Zotero/storage/J3XFCXWY/AGI and Lock-in.pdf:application/pdf;Snapshot:/Users/gat/Zotero/storage/3Y3FPGLB/www.forethought.org.html:text/html},
}

@online{davidson_ai-enabled_2025,
	title = {{AI}-Enabled Coups: How a Small Group Could Use {AI} to Seize Power},
	url = {https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power},
	abstract = {The development of {AI} that is more broadly capable than humans will create a new and serious threat: *{AI}-enabled coups*. An {AI}-enabled coup could be staged by a very small group, or just a single person, and could occur even in established democracies. Sufficiently advanced {AI} will introduce three novel dynamics that significantly increase coup risk. Firstly, military and government leaders could fully replace human personnel with {AI} systems that are *singularly loyal* to them, eliminating the need to gain human supporters for a coup. Secondly, leaders of {AI} projects could deliberately build {AI} systems that are *secretly loyal* to them, for example fully autonomous military robots that pass security tests but later execute a coup when deployed in military settings. Thirdly, senior officials within {AI} projects or the government could gain *exclusive access* to superhuman capabilities in weapons development, strategic planning, persuasion, and cyber offense, and use these to increase their power until they can stage a coup. To address these risks, {AI} projects should design and enforce rules against {AI} misuse, audit systems for secret loyalties, and share frontier {AI} systems with multiple stakeholders. Governments should establish principles for government use of advanced {AI}, increase oversight of frontier {AI} projects, and procure {AI} for critical systems from multiple independent providers.},
	titleaddon = {Forethought},
	author = {Davidson, Tom and Finnveden, Lukas and Hadshar, Rose},
	urldate = {2025-07-03},
	date = {2025-04},
	langid = {english},
	file = {PDF:/Users/gat/Zotero/storage/SKF2S5C6/AI-Enabled Coups How a Small Group Could Use AI to Seize Power.pdf:application/pdf;Snapshot:/Users/gat/Zotero/storage/ENS63YD3/www.forethought.org.html:text/html},
}

@online{murphy_generating_2025,
	title = {Generating Wargames},
	url = {https://dennismurphy.substack.com/p/generating-wargames},
	abstract = {Leveraging {LLMs} for Red Team Emulation - {PME}, Routine Diplomacy, and Impersonation},
	titleaddon = {Atlanta Grand Strategy},
	type = {Substack newsletter},
	author = {Murphy, Dennis},
	urldate = {2025-07-04},
	date = {2025-04-13},
}
